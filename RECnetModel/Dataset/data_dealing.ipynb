{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       unique_id                        aa_seq  label\n",
      "0          uid_0      ATDALMTGY|CAISESQGNTEAFF      1\n",
      "1          uid_1      ATDALMTGY|CAISEDRALVSYTF      1\n",
      "2          uid_2      ATDALMTGY|CAISEDRALNEQFF      1\n",
      "3          uid_3      ATDALMTGY|CAVQPGQGMQPQHF      1\n",
      "4          uid_4     ATDALMTGY|CAISEGAMGNQPQHF      1\n",
      "...          ...                           ...    ...\n",
      "22277  uid_22277     NLVPMVATV|CASRAGTGYYNEQFF      0\n",
      "22278  uid_22278      NLVPMVATV|CASKRGVGEDTQYF      0\n",
      "22279  uid_22279      NLVPMVATV|CASSLPRTRDTQYF      0\n",
      "22280  uid_22280    NLVPMVATV|CASSYSGQGSSYGYTF      0\n",
      "22281  uid_22281  NLVPMVATV|CASSRLPATGGVTQPQHF      0\n",
      "\n",
      "[22282 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "把标准格式转化为xTrimol格式。\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "def process_csv(file_path, col1_name, col2_name, col3_name, output_file_path):\n",
    "    # 1. 使用 pandas 读取一个 csv\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 去除第一列和第二列字符串中的分号\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].astype(str).str.replace(';', '')\n",
    "    df.iloc[:, 1] = df.iloc[:, 1].astype(str).str.replace(';', '')\n",
    "    # 2. 将第一列的 str 和第二列的 str 用“|”合并，保存在第二列\n",
    "    df.iloc[:, 1] = df.iloc[:, 0].astype(str) + '|' + df.iloc[:, 1].astype(str)\n",
    "\n",
    "    # 3. 原先的第一列替换为 uid_0, uid_1 这种记行的信息\n",
    "    df.iloc[:, 0] = [f'uid_{i}' for i in range(len(df))]\n",
    "\n",
    "    # 4. 最后把列名改了，总共三列，列名自定义\n",
    "    df.columns = [col1_name, col2_name, col3_name]\n",
    "    \n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 示例使用\n",
    "file_path = '/home/users/hcdai/AI-peptide/Seq2Score/Henya/dataset/Example_majority2.csv'  # 替换为你的 CSV 文件路径\n",
    "col1_name = 'unique_id'\n",
    "col2_name = 'aa_seq'\n",
    "col3_name = 'label'\n",
    "output_file_path = '/home/users/hcdai/AI-peptide/Seq2Score/Henya/dataset/xTrimo_tcr_train.csv'\n",
    "\n",
    "result_df = process_csv(file_path, col1_name, col2_name, col3_name, output_file_path)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "'''\n",
    "RunRosetta.py,试图改成可提取序列的\n",
    "'''\n",
    "import time\n",
    "# import json\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB import Selection, PDBIO\n",
    "\n",
    "# Load the config JSON file\n",
    "# config_path = \"/home/users/hcdai/AI-peptide/RunRosetta/config.json\"\n",
    "\n",
    "workspace = ''\n",
    "index_csv_path = ''\n",
    "# with open(config_path, \"r\") as f:\n",
    "#     config = json.load(f)\n",
    "# workspace = config[\"workspace\"]\n",
    "# rosetta_path = config[\"rosetta\"]\n",
    "os.chdir(workspace)\n",
    "index_csv_pd = pd.read_csv(index_csv_path,index_col = 0)\n",
    "\n",
    "# %%\n",
    "# 读取pdb文件的索引信息\n",
    "pdb_index = pd.read_csv(config['pdb_index'], index_col=0)\n",
    "print(pdb_index.head())\n",
    "# ab = pdb_index.loc['1wej'][\"ligand_chain\"].split(';')\n",
    "# print(ab)\n",
    "\n",
    "# %%\n",
    "# 定义待处理的pdb文件的生成器\n",
    "\n",
    "pdb_dir = config['input']\n",
    "pdb_file_list = os.listdir(pdb_dir)\n",
    "pdb_file_num = len(pdb_file_list)\n",
    "\n",
    "def pdb_generator():\n",
    "    \"pdb文件生成器，用于input文件夹中读取pdb文件，仅限后缀为.pdb的文件\"\n",
    "    for i in range(pdb_file_num):\n",
    "        pdb_file:str = os.path.join(pdb_dir, pdb_file_list[i])\n",
    "        if os.path.isfile(pdb_file) and pdb_file.endswith('.pdb'):\n",
    "            yield pdb_file\n",
    "        else:\n",
    "            print(f\"{pdb_file} is not a valid pdb file.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# 定义pdb文件处理的函数\n",
    "\n",
    "def pdb_parser(pdb_file: str):\n",
    "    \"\"\"\n",
    "    解析pdb文件，返回序列等信息\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取pdb文件的名字\n",
    "    pdb_part = pdb_file.split('/')[-1].split('.')[:-1]\n",
    "    pdb_name = pdb_part[0]\n",
    "\n",
    "    # 检测pdb_name是否在pdb_index中\n",
    "    if pdb_name not in pdb_index.index:\n",
    "        print(f\"{pdb_name} not in pdb_index\")\n",
    "        return pdb_name\n",
    "\n",
    "\n",
    "    receptor_chain:list = pdb_index.loc[pdb_name][\"receptor_chain\"].replace(' ', '').split(',')\n",
    "    ligand_chain:list = pdb_index.loc[pdb_name][\"ligand_chain\"].replace(' ', '').split(',')\n",
    "\n",
    "\n",
    "    # 定义pdb对象（解析器）\n",
    "    parser = PDBParser(PERMISSIVE=1) # PERMISSIV 标签表示一些与PDB文件相关的问题会被忽略（注意某些原子和/或残基会丢失）。\n",
    "    structure = parser.get_structure(pdb_name, pdb_file)\n",
    "\n",
    "    # 从pdb对象中选取特定的链，并解析其序列\n",
    "\n",
    "\n",
    " \n",
    "    # 定义氨基酸三个字符映射方式\n",
    "    mapping = {'ALA': 'A', \n",
    "               'ARG': 'R', \n",
    "               'ASN': 'N', \n",
    "               'ASP': 'D', \n",
    "               'CYS': 'C', \n",
    "               'GLN': 'Q', \n",
    "               'GLU': 'E', \n",
    "               'GLY': 'G', \n",
    "               'HIS': 'H', \n",
    "               'ILE': 'I', \n",
    "               'LEU': 'L', \n",
    "               'LYS': 'K', \n",
    "               'MET': 'M', \n",
    "               'PHE': 'F', \n",
    "               'PRO': 'P', \n",
    "               'SER': 'S', \n",
    "               'THR': 'T', \n",
    "               'TRP': 'W', \n",
    "               'TYR': 'Y', \n",
    "               'VAL': 'V',\n",
    "               'ACE': ''}\n",
    "    \n",
    "    # 初始化序列\n",
    "    receptor_seq = {}\n",
    "    ligand_seq = {}\n",
    "\n",
    "    for r in receptor_chain:\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                if chain.get_id() == r:\n",
    "                    receptor_seq.update({r:''.join([mapping.get(item,'') for item in [residue.get_resname().strip() for residue in chain]])})\n",
    "    for l in ligand_chain:\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                if chain.get_id() == l:\n",
    "                    ligand_seq.update({l:''.join([mapping.get(item,'') for item in [residue.get_resname().strip() for residue in chain]])})\n",
    "                    \n",
    "                    \n",
    "\n",
    "    return pdb_name , ligand_seq , receptor_seq  \n",
    "\n",
    "\n",
    "def run_command(command):\n",
    "    result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)\n",
    "    return result.stdout\n",
    "    \n",
    "\n",
    "# %%\n",
    "def rosetta_score_changed(pdb_file_path, output_dir, receptor_chain, ligand_chain, rosetta_path:dict = rosetta_path, ):\n",
    "    import os\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  \n",
    "    \n",
    "    print(\"Running Rosetta scoring\")\n",
    "    \n",
    "\n",
    "    # 执行命令的函数\n",
    "    # def run_command(command):\n",
    "    #     result = subprocess.run(command, shell=True, check=True, text=True, capture_output=True)\n",
    "    #     return result.stdout\n",
    "    \n",
    "    rosetta_temp_path = os.path.join(config[\"temp\"])\n",
    "    if not os.path.exists(rosetta_temp_path):\n",
    "        os.makedirs(rosetta_temp_path)\n",
    "    scorefile_path = os.path.join(output_dir, \"scores.sc\")\n",
    "    if os.path.exists(scorefile_path):\n",
    "        os.remove(scorefile_path)\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    \n",
    "    ori_path = os.getcwd()\n",
    "    print(ori_path)\n",
    "    # 下策：改变工作路径\n",
    "    os.chdir(rosetta_temp_path)\n",
    "    print('os.dir done')\n",
    " \n",
    "    # 对输入文件进行打分\n",
    "    score_command = f\"{rosetta_path['score_executable']} -s {pdb_file_path} -no_optH false -ignore_unrecognized_res -out:pdb\"\n",
    "    print(score_command)\n",
    "    run_command(score_command)\n",
    "    print(\"Rosetta score_command successful.\")\n",
    "\n",
    "    # 将rosetta_temp_path中打分后的文件重命名，为原名称后面加上\"_scored\"\n",
    "    pdb_files:list = [f for f in os.listdir(rosetta_temp_path) if f.endswith(\".pdb\")] # type: ignore\n",
    "    for pdb_file in pdb_files:\n",
    "        os.rename(os.path.join(rosetta_temp_path, pdb_file),\n",
    "                   os.path.join(rosetta_temp_path, \n",
    "                                pdb_file.replace(\".pdb\", \"_scored.pdb\")))    \n",
    "        \n",
    "    print(\"rename successful.\")\n",
    "    print(output_dir)\n",
    "\n",
    "    # 从rosetta_temp_path中逐条读取文件相对路径，进行接口分析，分析结果保存至output_dir中下以ligand_name命名的文件夹中\n",
    "    for pdb_file in pdb_files:\n",
    "        scored_pdb_file = pdb_file.replace(\".pdb\", \"_scored.pdb\")\n",
    "        scored_pdb_path = os.path.join(rosetta_temp_path, scored_pdb_file)\n",
    "        \n",
    "    # 进行接口分析，输出结果到指定的文件夹\n",
    "    analyze_command = f\"{rosetta_path['InterfaceAnalyzer']} -s {scored_pdb_path} -fixedchains {''.join(receptor_chain)},{''.join(ligand_chain)} @{rosetta_path['pack_input_options']}\"\n",
    "    run_command(analyze_command)\n",
    "\n",
    "    print(f\"InterfaceAnalyzer successful for {scored_pdb_file}.\")\n",
    "\n",
    "    # 复制temp文件夹内的所有文件文件到output_dir\n",
    "    for file in os.listdir(rosetta_temp_path):\n",
    "        # if file.endswith(\".pdb\") or file.endswith(\".sc\"):\n",
    "        if file.endswith(\".sc\"):\n",
    "            shutil.copy(os.path.join(rosetta_temp_path, file), os.path.join(output_dir, file))\n",
    "        \n",
    "\n",
    "    # 下策：重新定义回旧工作路径    \n",
    "    os.chdir(ori_path)\n",
    "    print(os.getcwd())\n",
    "\n",
    "    print(\"Rosetta scoring successful.\")\n",
    "\n",
    "# %%\n",
    "def csv_add(csv_file_path, score_output_dir, ligand_name, ligand_sequence, receptor_name, receptor_sequence):\n",
    "    '''将分数结果写入csv文件\n",
    "    Args:\n",
    "        csv_file_path: csv文件路径\n",
    "        ligand_name: 配体名称\n",
    "        ligand_sequence: 配体序列\n",
    "        receptor_name: 受体名称\n",
    "        receptor_sequence: 受体序列\n",
    "        score_output_dir: 分数输出文件夹路径\n",
    "    '''\n",
    "    pack_score_path=os.path.join(score_output_dir, \"pack_input_score.sc\")\n",
    "    scores_score_path=os.path.join(score_output_dir, \"score.sc\")\n",
    "    time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "    # 创建目录（如果不存在）\n",
    "    # csv_dir = os.path.dirname(csv_file_path)\n",
    "    # if not os.path.exists(csv_dir):\n",
    "    #     os.makedirs(csv_dir)\n",
    "\n",
    "    # 初始化结果列表\n",
    "    result_list = []\n",
    "\n",
    "    # 读取pack_input_scores.sc文件，获取打分数据\n",
    "    with open(pack_score_path, 'r') as sc_file:\n",
    "            lines = sc_file.readlines()\n",
    "            if len(lines) >= 3:\n",
    "                headers = [f\"pack_{header}\" for header in lines[1].strip().split()[1:]]  # 第二行为表头，去掉第一个，并添加前缀\n",
    "                data = lines[2].strip().split()     # 第三行为数据\n",
    "                # 去掉第一个表头和第一个数据\n",
    "                # headers = headers[1:]\n",
    "                data = data[1:]\n",
    "                # 创建字典存储打分数据\n",
    "                pack_score_data = dict(zip(headers, data))\n",
    "    # 获取scores.sc打分数据\n",
    "    with open(scores_score_path, 'r') as sc_file:\n",
    "        lines = sc_file.readlines()\n",
    "        if len(lines) >= 3:\n",
    "            headers = [f\"scores_{header}\" for header in lines[1].strip().split()[1:]]  # 第二行为表头，去掉第一个，并添加前缀\n",
    "           # 初始化一个字典来存储数据，键为表头，值为空列表\n",
    "        scores_data = {header: [] for header in headers}\n",
    "            # 从第三行开始读取数据（索引为2的行）\n",
    "        for line in lines[2:]:\n",
    "            # 分割每行的数据，并跳过第一个数据（索引为0的元素）\n",
    "            data = line.strip().split()[1:]\n",
    "            \n",
    "            # 检查数据长度是否与表头长度一致\n",
    "            if len(data) != len(headers):\n",
    "                raise ValueError(f\"数据长度 {len(data)} 与表头长度 {len(headers)} 不一致\")\n",
    "            \n",
    "            # 将数据添加到对应的表头列表中\n",
    "            for i, header in enumerate(headers):\n",
    "                scores_data[header].append(data[i])\n",
    "    \n",
    "    # 合并打分数据\n",
    "    merged_scores_data = {**scores_data, **pack_score_data}\n",
    "\n",
    "    result_list.append({\n",
    "        'ligand_name': ligand_name,\n",
    "        'ligand_sequence': ligand_sequence,\n",
    "        'receptor_name': receptor_name,\n",
    "        'receptor_sequence': receptor_sequence, \n",
    "        'time': time,\n",
    "        **merged_scores_data  # 将打分数据作为额外字段添加\n",
    "    })\n",
    "\n",
    "    # 写入csv文件\n",
    "    with open(csv_file_path, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['ligand_name', 'ligand_sequence', 'receptor_name', 'receptor_sequence','time']\n",
    "        # 添加打分数据的表头\n",
    "        for key in result_list[0].keys():\n",
    "            if key not in fieldnames:\n",
    "                fieldnames.append(key)\n",
    "        \n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        # writer.writeheader()\n",
    "        for row in result_list:\n",
    "            writer.writerow(row)\n",
    " \n",
    "    print(f\"Results have been written to {csv_file_path}\")\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    shutil.rmtree(config[\"temp\"])\n",
    "    print(\"Cleaned up temp directory.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 新建error_pdb.txt文件\n",
    "try:\n",
    "    with open(config[\"error_pdb\"], 'w') as file:\n",
    "        pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not os.path.isfile(config[\"output_csv\"]):\n",
    "    with open(config[\"output_csv\"], mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            fieldnames = ['ligand_name','ligand_sequence','receptor_name','receptor_sequence','time','scores_total_score','scores_dslf_fa13','scores_fa_atr','scores_fa_dun','scores_fa_elec','scores_fa_intra_rep','scores_fa_intra_sol_xover4','scores_fa_rep','scores_fa_sol','scores_hbond_bb_sc','scores_hbond_lr_bb','scores_hbond_sc','scores_hbond_sr_bb','scores_linear_chainbreak','scores_lk_ball_wtd','scores_omega','scores_overlap_chainbreak','scores_p_aa_pp','scores_pro_close','scores_rama_prepro','scores_ref','scores_yhh_planarity','scores_description','pack_total_score','pack_complex_normalized','pack_dG_cross','pack_dG_cross/dSASAx100','pack_dG_separated','pack_dG_separated/dSASAx100','pack_dSASA_hphobic','pack_dSASA_int','pack_dSASA_polar','pack_delta_unsatHbonds','pack_dslf_fa13','pack_fa_atr','pack_fa_dun','pack_fa_elec','pack_fa_intra_rep','pack_fa_intra_sol_xover4','pack_fa_rep','pack_fa_sol','pack_hbond_E_fraction','pack_hbond_bb_sc','pack_hbond_lr_bb','pack_hbond_sc','pack_hbond_sr_bb','pack_hbonds_int','pack_lk_ball_wtd','pack_nres_all','pack_nres_int','pack_omega','pack_p_aa_pp','pack_packstat','pack_per_residue_energy_int','pack_pro_close','pack_rama_prepro','pack_ref','pack_sc_value','pack_side1_normalized','pack_side1_score','pack_side2_normalized','pack_side2_score','pack_yhh_planarity','pack_description']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "# %%\n",
    "# 实例化生成器\n",
    "\n",
    "for pdb_file_path in pdb_generator():\n",
    "    parser = tuple(pdb_parser(pdb_file_path))\n",
    "    print(pdb_file_path)\n",
    "    print(pdb_parser(pdb_file_path))  \n",
    "    print(parser)\n",
    "\n",
    "    if len(parser) != 3:\n",
    "        # 以追加模式将问题pdb写入error_pdb.txt文件\n",
    "        with open('/home/users/hcdai/AI-peptide/RunRosetta/error_pdb.txt', 'a') as f:\n",
    "            f.write(\"\".join(list(parser)) + '\\n')\n",
    "        continue\n",
    "        \n",
    "    pdb_name, ligand_seq, receptor_seq = parser\n",
    "\n",
    "    output_path = os.path.join(config['output'], pdb_name)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    # 调用Rosetta的程序\n",
    "    # try:\n",
    "    #     time.sleep(1)\n",
    "    #     print(\"sleep ended\")\n",
    "    rosetta_score_changed(\n",
    "        pdb_file_path,\n",
    "        output_path,\n",
    "        receptor_chain = pdb_index.loc[pdb_name][\"receptor_chain\"].split(';'),\n",
    "        ligand_chain = pdb_index.loc[pdb_name][\"ligand_chain\"].split(';'),\n",
    "        rosetta_path=config['rosetta'],\n",
    "        \n",
    "    )\n",
    "    # except:\n",
    "    #     # 以追加模式将问题pdb写入error_pdb.txt文件\n",
    "    #     print(\"Rosetta scoring failed.\")\n",
    "    #     with open('/home/users/hcdai/AI-peptide/RunRosetta/error_pdb.txt', 'a') as f:\n",
    "    #         f.write(\"\".join(list(parser[0])) + \"255\"+ '\\n' )\n",
    "    #     continue\n",
    "            \n",
    "\n",
    "    # 写入csv文件\n",
    "    csv_add(\n",
    "        csv_file_path=config['output_csv'],\n",
    "        score_output_dir=output_path,\n",
    "        ligand_name = pdb_name,\n",
    "        ligand_sequence = ligand_seq,\n",
    "        receptor_name = pdb_name,  # !!!!!!!!!!!!!!!!!\n",
    "        receptor_sequence = receptor_seq,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(config[\"temp\"])\n",
    "        print(\"Cleaned up temp directory.\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mUnexpected end of JSON input. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "from Bio.PDB import Selection, PDBIO\n",
    "\n",
    "'''\n",
    "args:\n",
    "'''\n",
    "workspace = '/home/users/hcdai/AI-peptide/Seq2Score/Henya/dataset/PPI-Affinity/PDB'\n",
    "index_csv_path = '/home/users/hcdai/AI-peptide/Seq2Score/Henya/dataset/PPI-Affinity/PDB/PPB-Affinity.csv'\n",
    "pdb_dir = ''\n",
    "\n",
    "'''\n",
    "vars:\n",
    "'''\n",
    "os.chdir(workspace)\n",
    "index_csv_pd = pd.read_csv(index_csv_path,index_col = 0)\n",
    "print(pdb_index.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！生成的文件：\n",
      "FASTA文件: /home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/finetuning_test/xcapt_forcompare.fasta\n",
      "TSV文件: /home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/finetuning_test/xcapt_forcompare.tsv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "适应xCAPT的调整\n",
    "'''\n",
    "import csv\n",
    "\n",
    "# 输入输出文件路径\n",
    "input_csv = \"/home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/finetuning_test/for_compare.csv\"\n",
    "output_fasta = \"/home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/finetuning_test/xcapt_forcompare.fasta\"\n",
    "output_tsv = \"/home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/finetuning_test/xcapt_forcompare.tsv\"\n",
    "\n",
    "# 初始化计数器\n",
    "pep_counter = 1\n",
    "cdr_counter = 1\n",
    "\n",
    "# 打开文件并处理\n",
    "with open(input_csv, 'r') as csv_in, \\\n",
    "     open(output_fasta, 'w') as fasta_out, \\\n",
    "     open(output_tsv, 'w', newline='') as tsv_out:\n",
    "    \n",
    "    # 创建CSV读取器\n",
    "    reader = csv.reader(csv_in)\n",
    "    next(reader)  # 跳过标题行\n",
    "    \n",
    "    # 创建TSV写入器\n",
    "    tsv_writer = csv.writer(tsv_out, delimiter='\\t')\n",
    "    \n",
    "    # 遍历每一行数据\n",
    "    for row in reader:\n",
    "        # 解析各列数据\n",
    "        pep_seq = row[0]\n",
    "        cdr_seq = row[1]\n",
    "        label = row[2]\n",
    "        \n",
    "        # 生成序列名称\n",
    "        pep_name = f\"pep-{pep_counter}\"\n",
    "        cdr_name = f\"cdr-{cdr_counter}\"\n",
    "        \n",
    "        # 写入FASTA文件\n",
    "        fasta_out.write(f\">{pep_name}\\n{pep_seq}\\n\")\n",
    "        fasta_out.write(f\">{cdr_name}\\n{cdr_seq}\\n\")\n",
    "        \n",
    "        # 写入TSV文件\n",
    "        tsv_writer.writerow([pep_name, cdr_name, label])\n",
    "        \n",
    "        # 计数器递增\n",
    "        pep_counter += 1\n",
    "        cdr_counter += 1\n",
    "\n",
    "print(\"处理完成！生成的文件：\")\n",
    "print(f\"FASTA文件: {output_fasta}\")\n",
    "print(f\"TSV文件: {output_tsv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# 如果全为空，跳过当前行\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 解析各列数据\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m pep_seq \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpep_seq_col\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m cdr_seq \u001b[38;5;241m=\u001b[39m row[cdr_seq_col]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_label:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rec'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "适应xCAPT的调整2.0，增加了指定列名和默认全1的功能\n",
    "'''\n",
    "import csv\n",
    "\n",
    "# 输入输出文件路径\n",
    "input_csv = \"/home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/target_pep/peps_fintuning_tset.csv\"\n",
    "output_fasta = \"/home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/target_pep/peps_fintuning_tset_forcompare.fasta\"\n",
    "output_tsv = \"/home/users/hcdai/AI-peptide/Seq2Score/xCAPT5/models/MCAPST5-X/protT5/seq/test/target_pep/peps_fintuning_tset_forcompare.tsv\"\n",
    "\n",
    "# 指定列名\n",
    "pep_seq_col = 'Peptide_seq'  # 替换为实际的列名\n",
    "cdr_seq_col = 'Receptor_seq'  # 替换为实际的列名\n",
    "label_col = 'TF(T=1)'      # 替换为实际的列名\n",
    "\n",
    "# 是否读取label的标志\n",
    "read_label = True  # 可以修改为False来不读取label，全部默认为1\n",
    "\n",
    "# 初始化计数器\n",
    "pep_counter = 1\n",
    "cdr_counter = 1\n",
    "\n",
    "# 打开文件并处理\n",
    "with open(input_csv, 'r') as csv_in, \\\n",
    "     open(output_fasta, 'w') as fasta_out, \\\n",
    "     open(output_tsv, 'w', newline='') as tsv_out:\n",
    "    \n",
    "    # 创建CSV读取器\n",
    "    reader = csv.DictReader(csv_in)\n",
    "    \n",
    "    # 创建TSV写入器\n",
    "    tsv_writer = csv.writer(tsv_out, delimiter='\\t')\n",
    "    \n",
    "    # 遍历每一行数据\n",
    "    for row in reader:\n",
    "        # 检查当前行是否全为空\n",
    "        if all(value.strip() == '' for value in row.values()):\n",
    "            continue  # 如果全为空，跳过当前行\n",
    "        # 解析各列数据\n",
    "        pep_seq = row[pep_seq_col]\n",
    "        cdr_seq = row[cdr_seq_col]\n",
    "        if read_label:\n",
    "            label = row[label_col]\n",
    "        else:\n",
    "            label = '1'\n",
    "        \n",
    "        # 生成序列名称\n",
    "        pep_name = f\"pep-{pep_counter}\"\n",
    "        cdr_name = f\"cdr-{cdr_counter}\"\n",
    "        \n",
    "        # 写入FASTA文件\n",
    "        fasta_out.write(f\">{pep_name}\\n{pep_seq}\\n\")\n",
    "        fasta_out.write(f\">{cdr_name}\\n{cdr_seq}\\n\")\n",
    "        \n",
    "        # 写入TSV文件\n",
    "        tsv_writer.writerow([pep_name, cdr_name, label])\n",
    "        \n",
    "        # 计数器递增\n",
    "        pep_counter += 1\n",
    "        cdr_counter += 1\n",
    "\n",
    "print(\"处理完成！生成的文件：\")\n",
    "print(f\"FASTA文件: {output_fasta}\")\n",
    "print(f\"TSV文件: {output_tsv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
