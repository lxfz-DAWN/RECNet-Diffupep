vocab_size: 512 # 设置词汇表的大小，决定了模型可以处理的不同词语或token的数量
hidden_size: 1152 # 4096
num_filters: 1152 # 4096
max_sequence_len: 8192
attn_layer_idxs: [8, 16, 24,]
hyena_layer_idxs: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27,]
num_layers: 28
short_filter_length: 3  
num_attention_heads: 32
short_filter_bias: True
mlp_init_method: torch.nn.init.normal_ # torch.nn.init.zeros_
mlp_output_init_method: torch.nn.init.normal_ # 这里调的是初始化分布
eps: 1.0e-6 
state_size: 8 
inner_size_multiple_of: 16  # force GLU inner_size to be a multiple of
smeared_gqa: False  
make_vocab_size_divisible_by: 8
log_intermediate_values: False
proj_groups: 1  # GQA
hyena_filter_groups: 1  
split_k0: True  
model_parallel_size: 1 # 模型并行！！！！！！！！！！！
pile_parallel_size: 1
tie_embeddings: True  # 如果为 True，则 unembed 与 embedding_layer 共享参数。
inner_mlp_size: null  # set to None, so it auto-fills
mha_out_proj_bias: True
qkv_proj_bias: True
final_norm: True  
rng_fork: False
use_flash_attn: True
use_flash_rmsnorm: False  
use_flash_depthwise: False 
use_flashfft: False  # 原本为False
column_split: True  # only affects outputs when proj_groups > 1
inference_mode: True   # 原本为True
tokenizer_type: CharLevelTokenizer  
prefill_style: fft  
mlp_activation: gelu