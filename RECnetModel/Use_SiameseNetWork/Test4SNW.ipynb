{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.datasets as dset \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable   \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319274/1431351066.py:5: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  plt.legend()  # 添加图例\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # git上的示例代码，可以调用ESMC模型进行embedding\n",
    "# protein = ESMProtein(sequence=\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC\")\n",
    "# client = ESMC.from_pretrained(\"esmc_600m\").to(\"cpu\") # or \"cpu\"\n",
    "# protein_tensor = client.encode(protein)\n",
    "# logits_output = client.logits(\n",
    "#    protein_tensor, LogitsConfig(sequence=True, return_embeddings=True,return_hidden_states=True)\n",
    "# )\n",
    "# print(logits_output.hidden_states.shape)\n",
    "# # 这个hidden_states就是输出的隐层特征\n",
    "\n",
    "def ESMC_embedding(sequence:str):\n",
    "   '''调用ESMC模型进行embedding\n",
    "   \n",
    "   输入：蛋白质序列\n",
    "   输出：embedding后的隐层特征（Size = [36, 1, 68, 1152]）\n",
    "   '''\n",
    "   protein = ESMProtein(sequence=sequence)\n",
    "   client = ESMC.from_pretrained(\"esmc_600m\").to(\"cpu\") # or \"cpu\"\n",
    "   protein_tensor = client.encode(protein)\n",
    "   logits_output = client.logits(protein_tensor, LogitsConfig(sequence=True, return_embeddings=True, return_hidden_states=True))\n",
    "   return logits_output.hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transformer 部分：提取全局特征\n",
    "# self.transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "#     d_model=128,  # 输入特征维度\n",
    "#     nhead=8,      # 多头注意力头数\n",
    "#     dim_feedforward=512,  # 前馈网络隐藏层维度\n",
    "#     dropout=0.1   # Dropout 概率\n",
    "# )\n",
    "# self.transformer_encoder = nn.TransformerEncoder(\n",
    "#     self.transformer_encoder_layer, num_layers=2  # Transformer 层数\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     '''\n",
    "#     使用 Transformer 架构和多头注意力机制进行序列特征提取，输出特征提取后的信息。\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, embedding_dim = 1152, NormalizedSequenceLength = 50,):\n",
    "#         \"\"\"\n",
    "#         初始化 Transformer 架构。\n",
    "\n",
    "#         参数:\n",
    "#             embedding_dim (int): 输入嵌入的维度（默认 1152）。\n",
    "#         \"\"\"\n",
    "#         super(SiameseNetwork, self).__init__()\n",
    "\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.NormalizedSequenceLength = NormalizedSequenceLength\n",
    "\n",
    "\n",
    "#         self.transformer_encoder_layer1 = nn.TransformerEncoderLayer(\n",
    "#             d_model = self.embedding_dim,  # 输入特征维度\n",
    "#             nhead=8,      # 多头注意力头数\n",
    "#             dim_feedforward=512,  # 前馈网络隐藏层维度\n",
    "#             dropout=0.1,  # 丢弃率\n",
    "#             activation='relu'  # 激活函数\n",
    "#         )\n",
    "\n",
    "#         self.transformer_encoder1 = nn.TransformerEncoder(\n",
    "#             self.transformer_encoder_layer1,  # 编码层\n",
    "#             num_layers=2  # 编码层数\n",
    "#         )\n",
    "\n",
    "#         self.linear_layers1 = nn.Sequential(\n",
    "#             nn.Linear(self.NormalizedSequenceLength * 36, 512),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "        \n",
    "\n",
    "#         self.linear_layers2 = nn.Sequential(\n",
    "#             nn.Linear(self.embedding_dim, 512),\n",
    "#             nn.ReLU()\n",
    "#             )\n",
    "    \n",
    "#         self.transformer_encoder_layer2 = nn.TransformerEncoderLayer(\n",
    "#         d_model = 512,  # 输入特征维度\n",
    "#         nhead=8,      # 多头注意力头数\n",
    "#         dim_feedforward=512,  # 前馈网络隐藏层维度\n",
    "#         dropout=0.1,  # 丢弃率\n",
    "#         activation='relu'  # 激活函数\n",
    "#         )\n",
    "\n",
    "#         self.transformer_encoder2 = nn.TransformerEncoder(\n",
    "#             self.transformer_encoder_layer1,  # 编码层\n",
    "#             num_layers=2  # 编码层数\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def _forward_ligand(self, x:torch.Tensor):\n",
    "#         '''\n",
    "#         前向传播。\n",
    "\n",
    "#         参数:\n",
    "#             x (torch.Tensor): 输入张量，形状为 [batch_size, ESMC_size, 1, sequence_length, embedding_dim]。\n",
    "\n",
    "#         返回:\n",
    "#             torch.Tensor: 输出张量，形状为 \n",
    "#         '''\n",
    "#         x = x.squeeze(2)  # 去掉第二维度 [batch_size, ESMC_size, sequence_length, embedding_dim]\n",
    "#         x = x.reshape(x.size(0), -1, x.size(-1))  # 展平 [batch_size, ESMC_size * sequence_length, embedding_dim]\n",
    "#         x = x.permute(1, 0, 2)  # 转置 [ESMC_size * sequence_length, batch_size, embedding_dim]\n",
    "#         x = self.transformer_encoder1(x)  # 编码器  [ESMC_size * sequence_length, batch_size, embedding_dim] [1800, 32, 1152]\n",
    "#         x = x.permute(1, 2, 0)  # 转置 [batch_size, embedding_dim, ESMC_size * sequence_length]\n",
    "#         x = self.linear_layers1(x)  # 全连接层 [batch_size, embedding_dim , 512]\n",
    "#         x = x.permute(2, 0, 1)  # [512, batch_size, embedding_dim]\n",
    "#         x = self.transformer_encoder1(x)  # 编码器  [512, batch_size, embedding_dim]\n",
    "#         x = self.linear_layers2(x) # 全连接层 [512, batch_size, 512]\n",
    "#         x = self.transformer_encoder_layer2(x)  # 编码器  [512, batch_size, 512]\n",
    "#         x = x.permute(1, 0, 2)   # [batch_size, 512, 512]\n",
    "#         print(x.shape)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#         return x\n",
    "    \n",
    "\n",
    "#     def _forward_receptor(self, x:torch.Tensor):\n",
    "#         '''\n",
    "#         前向传播。\n",
    "\n",
    "#         参数:\n",
    "#             x (torch.Tensor): 输入张量，形状为 [batch_size, ESMC_size, 1, sequence_length, embedding_dim]。\n",
    "\n",
    "#         返回:            \n",
    "#             torch.Tensor: 输出张量，形状为\n",
    "#         '''\n",
    "#         x = x.squeeze(2)  # 去掉第二维度 [batch_size, ESMC_size, sequence_length, embedding_dim]\n",
    "#         x = x.reshape(x.size(0), -1, x.size(-1))  # 展平 [batch_size, ESMC_size * sequence_length, embedding_dim]\n",
    "#         x = x.permute(1, 0, 2)  # 转置 [ESMC_size * sequence_length, batch_size, embedding_dim]\n",
    "#         x = self.transformer_encoder1(x)  # 编码器  [ESMC_size * sequence_length, batch_size, embedding_dim] [1800, 32, 1152]\n",
    "#         x = x.permute(1, 2, 0)  # 转置 [batch_size, embedding_dim, ESMC_size * sequence_length]\n",
    "#         x = self.linear_layers1(x)  # 全连接层 [batch_size, embedding_dim , 512]\n",
    "#         x = x.permute(2, 0, 1)  # [512, batch_size, embedding_dim]\n",
    "#         x = self.transformer_encoder1(x)  # 编码器  [512, batch_size, embedding_dim]\n",
    "#         x = self.linear_layers2(x) # 全连接层 [512, batch_size, 512]\n",
    "#         x = self.transformer_encoder_layer2(x)  # 编码器  [512, batch_size, 512]\n",
    "#         x = x.permute(1, 0, 2)   # [batch_size, 512, 512]\n",
    "#         print(x.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "#     def forward(self, input1, input2):\n",
    "#         output1 = self._forward_ligand(input1)\n",
    "#         output2 = self._forward_receptor(input2)\n",
    "#         return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label, distances = 'cosine_distance'):\n",
    "        # label指示两个样本是否相似（1表示不相似，0表示相似）\n",
    "        if distances == 'euclidean_distance':\n",
    "            # 欧几里得距离：适合低维稠密数据，捕捉绝对差异\n",
    "            euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "            loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                        (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        elif distances == 'cosine_distance':\n",
    "            # 余弦距离：适合高维稀疏数据，方向敏感，适合高维稀疏矩阵\n",
    "            cosine_similarity = F.cosine_similarity(output1, output2, dim=1, eps=1e-6)\n",
    "            cosine_distance = 1 - cosine_similarity\n",
    "            loss_contrastive = torch.mean((1-label) * torch.pow(cosine_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - cosine_distance, min=0.0), 2))\n",
    "        elif distances == 'exponential_distance':\n",
    "            # 指数距离：适合需要强调微小差异的任务\n",
    "            euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "            exponential_distance = torch.exp(euclidean_distance) - 1\n",
    "            loss_contrastive = torch.mean((1-label) * torch.pow(exponential_distance, 2) +\n",
    "                                        (label) * torch.pow(torch.clamp(self.margin - exponential_distance, min=0.0), 2))\n",
    "            # 如果两个输入不相似的程度大于self.margin，即足够不相似，则loss为0，否则loss为平方距离\n",
    "        else:\n",
    "            raise ValueError('Unsupported distance metric: {}'.format(distances))\n",
    "\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "class ProteinPairsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, NormalizedSequenceLength, Seq2Tensor_path = './Seq2Tensor', transform=None):\n",
    "        \"\"\"\n",
    "        初始化数据集。\n",
    "\n",
    "        参数:\n",
    "            data (DataFrame): 包含序列对及其标签的 DataFrame。\n",
    "            Seq2Tensor_path (str): 存储张量数据的路径。\n",
    "        \"\"\"\n",
    "        self.data = data  \n",
    "        self.NormalizedSequenceLength = NormalizedSequenceLength\n",
    "        self.Seq2Tensor_path = Seq2Tensor_path\n",
    "        self.ProteinPairsDataTensors = set([path.split('/')[-1].rsplit('.', 1)[0] for path in glob.glob(f\"{Seq2Tensor_path}/*pt\")]) # 加载已经保存好的Emdeding数据\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "    def seq2tensor(self, seq):\n",
    "        if seq in self.ProteinPairsDataTensors:\n",
    "            return torch.load(f\"{self.Seq2Tensor_path}/{seq}.pt\")\n",
    "        else:\n",
    "            seq_tensor = ESMC_embedding(seq)\n",
    "            try:\n",
    "                torch.save(seq_tensor, f\"{self.Seq2Tensor_path}/{seq}.pt\")\n",
    "            except:\n",
    "                print(f\"Error: {seq} cannot be saved.\")\n",
    "            self.ProteinPairsDataTensors.add(seq)\n",
    "            return seq_tensor\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集的大小\"\"\"\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取特定索引的样本。\n",
    "\n",
    "        参数:\n",
    "            idx (int): 索引值\n",
    "\n",
    "        返回:\n",
    "            dict: 包含序列对及其标签的字典\n",
    "        \"\"\"\n",
    "        sequence1 = self.data.iloc[idx]['sequence1']  # 根据列名获取序列1\n",
    "        sequence2 = self.data.iloc[idx]['sequence2']  # 根据列名获取序列2\n",
    "        label = self.data.iloc[idx]['label']  # 根据列名获取标签\n",
    "\n",
    "        # 将序列转成张量，使用seq2tensor\n",
    "        sequence1_tensor = self.seq2tensor(sequence1)\n",
    "        sequence2_tensor = self.seq2tensor(sequence2)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sequence1_tensor = self.transform(sequence1_tensor, self.NormalizedSequenceLength)\n",
    "            sequence2_tensor = self.transform(sequence2_tensor, self.NormalizedSequenceLength)        \n",
    "\n",
    "        return {\"sequence1\": sequence1_tensor, \"sequence2\": sequence2_tensor, \"label\": label}\n",
    "\n",
    "def transform(sequence_tensor, NormalizedSequenceLength = 50):\n",
    "    \"\"\"\n",
    "    判断序列张量是否合规，否则进行变换\n",
    "    \"\"\"\n",
    "    # 输入的形状为[ESM_size, 1, sequence_length, embedding_dim]\n",
    "    # 输出的形状为[ESM_size, 1, NormalizedSequenceLength, embedding_dim]\n",
    "    if sequence_tensor.shape[2] > NormalizedSequenceLength:\n",
    "        sequence_tensor = sequence_tensor[:, :, :NormalizedSequenceLength, :]\n",
    "    elif sequence_tensor.shape[2] < NormalizedSequenceLength:\n",
    "        padding_tensor = torch.zeros(sequence_tensor.shape[0], sequence_tensor.shape[1], NormalizedSequenceLength - sequence_tensor.shape[2], sequence_tensor.shape[3])\n",
    "        sequence_tensor = torch.cat((sequence_tensor, padding_tensor), dim=2)\n",
    "    return sequence_tensor\n",
    "\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "def load_data(ProteinPairsData_csv_path, Seq2Tensor_path, NormalizedSequenceLength =50, batch_size=32, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    加载数据并划分为训练集、验证集和测试集。\n",
    "\n",
    "    参数:\n",
    "        ProteinPairsData_csv_path (str): CSV 文件路径。\n",
    "        batch_size (int): 每个批次的大小。\n",
    "        val_size (float): 验证集的比例（0-1之间的浮点数）。\n",
    "        test_size (float): 测试集的比例（0-1之间的浮点数）。\n",
    "\n",
    "    返回:\n",
    "        tuple: 训练集, 验证集和测试集的 DataLoader。\n",
    "    \"\"\"\n",
    "    # 读取 CSV 文件\n",
    "    data = pd.read_csv(ProteinPairsData_csv_path)\n",
    "\n",
    "    # 划分训练集和临时集（临时集中将会包含验证集和测试集）\n",
    "    train_data, temp_data = train_test_split(data, test_size=(val_size + test_size), random_state=42)\n",
    "\n",
    "    # 计算临时集中验证集和测试集的比例\n",
    "    temp_val_size = val_size / (val_size + test_size)\n",
    "    \n",
    "    # 划分验证集和测试集\n",
    "    val_data, test_data = train_test_split(temp_data, test_size=temp_val_size, random_state=42)\n",
    "\n",
    "    # 创建数据集实例\n",
    "    train_dataset = ProteinPairsDataset(data = train_data,\n",
    "                                        NormalizedSequenceLength = NormalizedSequenceLength,\n",
    "                                        Seq2Tensor_path = Seq2Tensor_path,\n",
    "                                        transform = transform)\n",
    "    val_dataset = ProteinPairsDataset(data = val_data,\n",
    "                                      NormalizedSequenceLength = NormalizedSequenceLength,\n",
    "                                      Seq2Tensor_path = Seq2Tensor_path,\n",
    "                                      transform = transform)\n",
    "    test_dataset = ProteinPairsDataset(data = test_data,\n",
    "                                       NormalizedSequenceLength = NormalizedSequenceLength,\n",
    "                                       Seq2Tensor_path = Seq2Tensor_path,\n",
    "                                       transform = transform)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = \"/home/users/hcdai/AI-peptide/Seq2Score/SiameseNetWork/est.csv\"\n",
    "# train_loader, val_loader, test_loader = load_data(csv_file_path,batch_size=1)\n",
    "# # 迭代训练集\n",
    "# for batch in train_loader:\n",
    "#     # print(batch[\"sequence1\"], batch[\"sequence2\"], batch[\"label\"])\n",
    "#     print(batch[\"sequence1\"].shape, batch[\"sequence2\"].shape, batch[\"label\"].shape)\n",
    "\n",
    "# # 迭代验证集\n",
    "# for batch in val_loader:\n",
    "#     print(batch[\"sequence1\"], batch[\"sequence2\"], batch[\"label\"])\n",
    "\n",
    "# # 迭代测试集\n",
    "# for batch in test_loader:\n",
    "#     print(batch[\"sequence1\"], batch[\"sequence2\"], batch[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础准备\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_plot(iteration,loss,label):\n",
    "    plt.figure(figsize=(10, 6))  # 设置图表大小\n",
    "    plt.plot(iteration, loss, color='blue', linewidth=2)  # 绘制损失曲线\n",
    "    plt.title(label, fontsize=16)  # 设置标题\n",
    "    plt.legend()  # 添加图例\n",
    "    plt.grid(True)  # 显示网格\n",
    "    plt.tight_layout()  # 自适应布局\n",
    "    plt.show()  # 显示图表\n",
    "\n",
    "# # 测试\n",
    "# iteration = range(1, 101)\n",
    "# loss = [i**2 for i in iteration]\n",
    "# label = 'Loss of Siamese Network'\n",
    "# show_plot(iteration, loss, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, file_path):\n",
    "    \"\"\"\n",
    "    保存模型及其状态\n",
    "\n",
    "    参数:\n",
    "        model: 要保存的模型\n",
    "        optimizer: 优化器\n",
    "        epoch: 当前训练的epoch\n",
    "        file_path: 保存的文件路径\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, file_path)\n",
    "\n",
    "def load_model(model, optimizer, file_path):\n",
    "    \"\"\"\n",
    "    加载模型及其状态\n",
    "\n",
    "    参数:\n",
    "        model: 要加载的模型\n",
    "        optimizer: 优化器\n",
    "        file_path: 加载的文件路径\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']  # 返回加载的epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练循环\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, margin, model_save_path=None):\n",
    "    \"\"\"\n",
    "    训练模型\n",
    "\n",
    "    参数:\n",
    "        model: 要训练的模型\n",
    "        train_loader: 用于训练的数据加载器\n",
    "        val_loader: 用于验证的数据加载器\n",
    "        num_epochs: 训练的轮数\n",
    "        learning_rate: 学习率\n",
    "        margin: 正负样本的margin值，用于loss计算\n",
    "    \"\"\"\n",
    "    # 设置优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = ContrastiveLoss(margin=margin)  # 定义对比损失类\n",
    "\n",
    "    train_losses = []  # 用于记录训练损失\n",
    "    val_losses = []    # 用于记录验证损失\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设置模型为训练模式\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # for batch in train_loader:\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch'): # 使用 tqdm 包装 train_loader，显示进度\n",
    "            sequence1 = batch[\"sequence1\"]\n",
    "            sequence2 = batch[\"sequence2\"]\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            optimizer.zero_grad()  # 清零梯度\n",
    "            \n",
    "            # 前向传播\n",
    "            output1, output2 = model(sequence1, sequence2)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # 验证\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        val_loss = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # 记录损失\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # 每个epoch后保存模型\n",
    "        if model_save_path:\n",
    "            save_model(model, optimizer, epoch, model_save_path)  # 每个epoch后保存模型\n",
    "\n",
    "        # # 可视化部分\n",
    "        # # 绘制损失曲线\n",
    "        # plt.figure()\n",
    "        # show_plot(range(num_epochs), train_losses, label='Training Loss')\n",
    "        # show_plot(range(num_epochs), val_losses, label='Validation Loss')\n",
    "\n",
    "    \n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    \"\"\"\n",
    "    验证模型\n",
    "\n",
    "    参数:\n",
    "        model: 要验证的模型\n",
    "        val_loader: 用于验证的数据加载器\n",
    "        criterion: 损失函数\n",
    "    \"\"\"\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # 评估时不需要反向传播\n",
    "        for batch in val_loader:\n",
    "            sequence1 = batch[\"sequence1\"]\n",
    "            sequence2 = batch[\"sequence2\"]\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            # 前向传播\n",
    "            output1, output2 = model(sequence1, sequence2)\n",
    "\n",
    "            # 计算损失\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# def test_model(model, test_loader):\n",
    "#     \"\"\"\n",
    "#     测试模型并计算测试损失和准确率。\n",
    "\n",
    "#     参数:\n",
    "#         model: 要测试的模型\n",
    "#         test_loader: 用于测试的数据加载器\n",
    "#         criterion: 损失函数\n",
    "\n",
    "#     返回:\n",
    "#         float: 测试损失\n",
    "#         float: 测试准确率\n",
    "#     \"\"\"\n",
    "#     model.eval()  # 设置模型为评估模式\n",
    "#     criterion = ContrastiveLoss()\n",
    "#     test_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     with torch.no_grad():  # 评估时不需要反向传播\n",
    "#         for batch in test_loader:\n",
    "#             sequence1 = batch[\"sequence1\"]\n",
    "#             sequence2 = batch[\"sequence2\"]\n",
    "#             labels = batch[\"label\"]\n",
    "\n",
    "#             # 前向传播\n",
    "#             output1, output2 = model(sequence1, sequence2)\n",
    "            \n",
    "#             # 计算损失\n",
    "#             loss = criterion(output1, output2, labels)\n",
    "#             test_loss += loss.item()\n",
    "\n",
    "#             # 计算准确率，这里假设通过某种方式获取每个样本的预测结果\n",
    "#             # 需要定义一个方法计算预测结果，比如使用阈值来决定正负样本\n",
    "#             predicted = (torch.nn.functional.cosine_similarity(output1, output2) > 0.5).float()  # 示例\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader)\n",
    "#     accuracy = correct / total * 100  # 转换为百分比\n",
    "\n",
    "#     print(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "#     return avg_test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# ProteinPairsData_csv_path = \"/home/users/hcdai/AI-peptide/Seq2Score/SiameseNetWork/est.csv\",\n",
    "# train_batch_size = 64,\n",
    "# train_number_epochs = 100,\n",
    "# learning_rate = 0.001, # learning_rate (float): SiameseNetwork的学习率\n",
    "# distances = \"cosine_distance\", # distances (str): 距离函数，可选[\"cosine_distance\", \"euclidean_distance\", \"exponential_distance\"]\n",
    "# margin = 2.0 # margin (float): 正负样本的margin值，用于loss计算\n",
    "# val_size = 0.1 # val_size (float): 验证集的比例（0-1之间的浮点数）\n",
    "# test_size = 0.1 # test_size (float): 测试集的比例（0-1之间的浮点数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/hcdai/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 1/100:   0%|          | 0/18 [00:00<?, ?batch/s]/tmp/ipykernel_266480/2243212197.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f\"{self.Seq2Tensor_path}/{seq}.pt\")\n",
      "Epoch 1/100:   0%|          | 0/18 [00:13<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 3317760000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 27\u001b[0m\n\u001b[1;32m     21\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 加载已保存的模型\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# load_model(model, optimizer, model_save_path)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSNW_Config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_number_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSNW_Config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSNW_Config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# # 测试模型\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     load_model(model, optimizer, model_load_path = model_save_path)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# # 测试模型\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# test_model(model, test_loader)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[42], line 36\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, margin, model_save_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清零梯度\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m output1, output2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output1, output2, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[36], line 76\u001b[0m, in \u001b[0;36mSiameseNetwork.forward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1, input2):\n\u001b[0;32m---> 76\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_ligand\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_receptor(input2)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output1, output2\n",
      "Cell \u001b[0;32mIn[36], line 49\u001b[0m, in \u001b[0;36mSiameseNetwork._forward_ligand\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# 展平 [batch_size, ESMC_size * sequence_length, embedding_dim]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 转置 [ESMC_size * sequence_length, batch_size, embedding_dim]\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 编码器  [ESMC_size * sequence_length, batch_size, embedding_dim] [1800, 32, 1152]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/transformer.py:904\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    900\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    903\u001b[0m         x\n\u001b[0;32m--> 904\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     )\n\u001b[1;32m    906\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/transformer.py:918\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sa_block\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    913\u001b[0m     x: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m     is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    917\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 918\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/ESMC/lib/python3.11/site-packages/torch/nn/functional.py:6278\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6275\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   6276\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 6278\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\n\u001b[1;32m   6280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6281\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   6282\u001b[0m     attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   6283\u001b[0m )\n\u001b[1;32m   6285\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 3317760000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# 加载config文件\n",
    "config_json_path = '/home/users/hcdai/AI-peptide/Seq2Score/SiameseNetWork/SNW_config.json'\n",
    "with open(config_json_path, 'r') as f:\n",
    "    SNW_Config = json.load(f)\n",
    "\n",
    "# 加载数据\n",
    "train_loader, val_loader, test_loader = load_data(ProteinPairsData_csv_path = SNW_Config[\"ProteinPairsData_csv_path\"], \n",
    "                                                  Seq2Tensor_path=SNW_Config[\"Seq2Tensor_path\"], \n",
    "                                                  NormalizedSequenceLength = SNW_Config[\"NormalizedSequenceLength\"], \n",
    "                                                  batch_size=32, \n",
    "                                                  val_size=0.1, \n",
    "                                                  test_size=0.1)\n",
    "\n",
    "# 实例化模型\n",
    "model = SiameseNetwork()\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 指定保存路径\n",
    "model_save_path = 'model_checkpoint.pth'\n",
    "\n",
    "# 加载已保存的模型\n",
    "# load_model(model, optimizer, model_save_path)\n",
    "\n",
    "# 训练模型\n",
    "train_model(model = model, \n",
    "            train_loader = train_loader, \n",
    "            val_loader = val_loader, \n",
    "            num_epochs = SNW_Config[\"train_number_epochs\"], \n",
    "            learning_rate = SNW_Config['learning_rate'], \n",
    "            margin = SNW_Config['margin'],\n",
    "            model_save_path = model_save_path)\n",
    "\n",
    "# # 测试模型\n",
    "# try:\n",
    "#     load_model(model, optimizer, model_load_path = model_save_path)\n",
    "# except FileNotFoundError:\n",
    "#     print('没有找到模型，要进行测试时，请确保模型文件存在。')\n",
    "\n",
    "# # 测试模型\n",
    "# test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"很经典的Unet网络\"\"\"\n",
    "\n",
    "    def __init__(self, input_nbr, label_nbr):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "        self.conv11 = nn.Conv2d(2*input_nbr, 16, kernel_size=3, padding=1)\n",
    "        self.bn11 = nn.BatchNorm2d(16)\n",
    "        self.do11 = nn.Dropout2d(p=0.2)\n",
    "        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(16)\n",
    "        self.do12 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(32)\n",
    "        self.do21 = nn.Dropout2d(p=0.2)\n",
    "        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(32)\n",
    "        self.do22 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(64)\n",
    "        self.do31 = nn.Dropout2d(p=0.2)\n",
    "        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(64)\n",
    "        self.do32 = nn.Dropout2d(p=0.2)\n",
    "        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn33 = nn.BatchNorm2d(64)\n",
    "        self.do33 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(128)\n",
    "        self.do41 = nn.Dropout2d(p=0.2)\n",
    "        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(128)\n",
    "        self.do42 = nn.Dropout2d(p=0.2)\n",
    "        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn43 = nn.BatchNorm2d(128)\n",
    "        self.do43 = nn.Dropout2d(p=0.2)\n",
    "\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn43d = nn.BatchNorm2d(128)\n",
    "        self.do43d = nn.Dropout2d(p=0.2)\n",
    "        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn42d = nn.BatchNorm2d(128)\n",
    "        self.do42d = nn.Dropout2d(p=0.2)\n",
    "        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn41d = nn.BatchNorm2d(64)\n",
    "        self.do41d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn33d = nn.BatchNorm2d(64)\n",
    "        self.do33d = nn.Dropout2d(p=0.2)\n",
    "        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn32d = nn.BatchNorm2d(64)\n",
    "        self.do32d = nn.Dropout2d(p=0.2)\n",
    "        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn31d = nn.BatchNorm2d(32)\n",
    "        self.do31d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.bn22d = nn.BatchNorm2d(32)\n",
    "        self.do22d = nn.Dropout2d(p=0.2)\n",
    "        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn21d = nn.BatchNorm2d(16)\n",
    "        self.do21d = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "\n",
    "        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.bn12d = nn.BatchNorm2d(16)\n",
    "        self.do12d = nn.Dropout2d(p=0.2)\n",
    "        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n",
    "\n",
    "        self.sm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        \"\"\"Forward method.\"\"\"\n",
    "        # Stage 1\n",
    "        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n",
    "        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n",
    "        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 2\n",
    "        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n",
    "        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n",
    "        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 3\n",
    "        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n",
    "        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n",
    "        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n",
    "        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n",
    "\n",
    "        # Stage 4\n",
    "        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n",
    "        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n",
    "        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n",
    "        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "        # Stage 4d\n",
    "        x4d = self.upconv4(x4p)\n",
    "        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n",
    "        x4d = torch.cat((pad4(x4d), x43), 1)\n",
    "        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n",
    "        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n",
    "        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n",
    "\n",
    "        # Stage 3d\n",
    "        x3d = self.upconv3(x41d)\n",
    "        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n",
    "        x3d = torch.cat((pad3(x3d), x33), 1)\n",
    "        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n",
    "        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n",
    "        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n",
    "\n",
    "        # Stage 2d\n",
    "        x2d = self.upconv2(x31d)\n",
    "        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n",
    "        x2d = torch.cat((pad2(x2d), x22), 1)\n",
    "        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n",
    "        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n",
    "\n",
    "        # Stage 1d\n",
    "        x1d = self.upconv1(x21d)\n",
    "        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n",
    "        x1d = torch.cat((pad1(x1d), x12), 1)\n",
    "        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n",
    "        x11d = self.conv11d(x12d)\n",
    "\n",
    "        output = []\n",
    "        output.append(x11d)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
