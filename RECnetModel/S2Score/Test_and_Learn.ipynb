{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    }
   ],
   "source": [
    "a = len('DIQMTQSPSSLSASVGDRVTITCRASGNIHNYLAWYQQKPGKAPKLLIYYTTTLADGVPSRFSGSGSGTDYTFTISSLQPEDIATYYCQHFWSTPRTFGQGTKVEIKRQVQLQESGPGLVRPSQTLSLTCTVSGFSLTGYGVNWVRQPPGRGLEWIGMIWGDGNTDYNSALKSRVTMLKDTSKNQFSLRLSSVTAADTAVYYCARERDYRLDYWGQGSLVTVSSKVFGRCELAAAMKRHGLDNYRGYSLGNWVCAAKFESNFNTQATNRNTDGSTDYGILQINSRWWCNDGRTPGSRNLCNIPCSALLSSDITASVNCAKKIVSDGNGMNAWVAWRNRCKGTDVQAWIRGCRL')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ligand_name', 'ligand_sequence', 'receptor_name', 'receptor_sequence',\n",
      "       'time', 'scores_total_score', 'scores_dslf_fa13', 'scores_fa_atr',\n",
      "       'scores_fa_dun', 'scores_fa_elec', 'scores_fa_intra_rep',\n",
      "       'scores_fa_intra_sol_xover4', 'scores_fa_rep', 'scores_fa_sol',\n",
      "       'scores_hbond_bb_sc', 'scores_hbond_lr_bb', 'scores_hbond_sc',\n",
      "       'scores_hbond_sr_bb', 'scores_linear_chainbreak', 'scores_lk_ball_wtd',\n",
      "       'scores_omega', 'scores_overlap_chainbreak', 'scores_p_aa_pp',\n",
      "       'scores_pro_close', 'scores_rama_prepro', 'scores_ref',\n",
      "       'scores_yhh_planarity', 'scores_description', 'pack_total_score',\n",
      "       'pack_complex_normalized', 'pack_dG_cross', 'pack_dG_cross/dSASAx100',\n",
      "       'pack_dG_separated', 'pack_dG_separated/dSASAx100',\n",
      "       'pack_dSASA_hphobic', 'pack_dSASA_int', 'pack_dSASA_polar',\n",
      "       'pack_delta_unsatHbonds', 'pack_dslf_fa13', 'pack_fa_atr',\n",
      "       'pack_fa_dun', 'pack_fa_elec', 'pack_fa_intra_rep',\n",
      "       'pack_fa_intra_sol_xover4', 'pack_fa_rep', 'pack_fa_sol',\n",
      "       'pack_hbond_E_fraction', 'pack_hbond_bb_sc', 'pack_hbond_lr_bb',\n",
      "       'pack_hbond_sc', 'pack_hbond_sr_bb', 'pack_hbonds_int',\n",
      "       'pack_lk_ball_wtd', 'pack_nres_all', 'pack_nres_int', 'pack_omega',\n",
      "       'pack_p_aa_pp', 'pack_packstat', 'pack_per_residue_energy_int',\n",
      "       'pack_pro_close', 'pack_rama_prepro', 'pack_ref', 'pack_sc_value',\n",
      "       'pack_side1_normalized', 'pack_side1_score', 'pack_side2_normalized',\n",
      "       'pack_side2_score', 'pack_yhh_planarity', 'TF', 'kd', 'IC50',\n",
      "       'pack_description'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/dataset/standard_csv_ori/test.csv')\n",
    "print(train_df.columns)\n",
    "# ligand_col_name = 'ligand_sequence'\n",
    "# with open('/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/dataset/standard_csv_withpath/test_withpath.csv', 'w') as f:\n",
    "#             # 写入表头\n",
    "#     header = ','.join(train_df.columns.tolist() + ['ESMC_Embedding_Path'])\n",
    "#     f.write(header + '\\n')\n",
    "\n",
    "#             # 逐行处理数据\n",
    "#     for index, row in train_df.iterrows():\n",
    "        \n",
    "#         ligand_seq = row[ligand_col_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mUnexpected end of JSON input. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个示例张量\n",
    "tensor = torch.randn(64, 65)\n",
    "\n",
    "# 裁剪前62位\n",
    "target_1 = tensor[:, :62]\n",
    "\n",
    "# 裁剪第63位\n",
    "target_2 = tensor[:, 62:63]\n",
    "\n",
    "# 裁剪第64位\n",
    "target_3 = tensor[:, 63:64]\n",
    "\n",
    "# 裁剪第65位\n",
    "target_4 = tensor[:, 64:65]\n",
    "\n",
    "print(\"target_1 shape:\", target_1.shape)\n",
    "print(\"target_2 shape:\", target_2.shape)\n",
    "print(\"target_3 shape:\", target_3.shape)\n",
    "print(\"target_4 shape:\", target_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2S_Loss():\n",
    "    '''\n",
    "    这个函数应该有的功能：\n",
    "        应对不同的cut形式\n",
    "        单独训练和加和训练两种模式\n",
    "    '''\n",
    "    def __init__(label_cut\n",
    "                 train_mode = 'sum'):\n",
    "        super().__init__()\n",
    "        self.label_cut = label_cut\n",
    "        self.train_mode = train_mode\n",
    "    \n",
    "    def cut(self,\n",
    "            model_output:torch_tensor,\n",
    "            label_cut):\n",
    "            \n",
    "    \n",
    "    def MSE_loss(self):    \n",
    "    \n",
    "    def total_loss(self, train_mode):\n",
    "        if train_mode == 'sum':\n",
    "            return 1\n",
    "        if train_mode == 'single':\n",
    "            return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class S2S_Loss():\n",
    "    '''\n",
    "    这个函数应该有的功能：\n",
    "        应对不同的cut形式\n",
    "        单独训练和加和训练两种模式\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 label_cut:list, #[62,1,1,1] # 前62个rosettascore和三个特殊变量\n",
    "                 train_target:list, # ['RosettaScore','kd','TF','IC50'],大小写区分清楚，只有kd小写\n",
    "                 adaptive_args:list,# 这玩意必须随着train_target调整啊，是一个必须调整的参数,长度必须为4，因为我实在是懒得接着写if了。eg：[0.3,0.3,0,0.3]\n",
    "                 ):\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.adaptive_args = adaptive_args\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def cut(self, model_output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        根据 label_cut 对模型输出进行裁剪\n",
    "        \"\"\"\n",
    "        cut_outputs = []\n",
    "        start = 0\n",
    "        for cut in self.label_cut:\n",
    "            end = start + cut\n",
    "            cut_outputs.append(model_output[:, start:end])\n",
    "            start = end\n",
    "        # cut_output = [[batch_size, 62], [batch_size, 1], [batch_size, 1], [batch_size, 1]] (if label_cut = [62, 1, 1, 1])\n",
    "        return cut_outputs\n",
    "\n",
    "    def MSE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算 MSE 损失\n",
    "        \"\"\"\n",
    "        return self.mse_loss_fn(output, target)\n",
    "\n",
    "    def BCE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算二元交叉熵损失\n",
    "        \"\"\"\n",
    "        return self.bce_loss_fn(output, target)\n",
    "\n",
    "    def total_loss(self, model_output, targets):\n",
    "        \"\"\"\n",
    "        根据 train_mode 计算总损失\n",
    "        \"\"\"\n",
    "        cut_outputs = self.cut(model_output)\n",
    "        loss_Rosetta = 0\n",
    "        loss_TF = 0\n",
    "        loss_kd = 0\n",
    "        loss_IC50 = 0\n",
    "        loss_content = []\n",
    "        total_loss = 0\n",
    "        a,b,c,d = self.adaptive_args\n",
    "        \n",
    "        for i in self.train_target:\n",
    "            if i == 'RosettaScore':\n",
    "                loss_Rossetta = self.MSE_loss(cut_outputs[0], targets[0])\n",
    "                loss_content.append('RosettaScore')\n",
    "                continue\n",
    "            if i == 'TF':\n",
    "                loss_TF = self.BCE_loss(cut_outputs[1], targets[1])\n",
    "                loss_content.append('TF')\n",
    "                continue\n",
    "            if i == 'kd':\n",
    "                loss_kd = self.MSE_loss(cut_outputs[2], targets[2])\n",
    "                loss_content.append('kd')\n",
    "                continue\n",
    "            if i == 'IC50':\n",
    "                loss_IC50 = self.BCE_loss(cut_outputs[3], targets[3])\n",
    "                loss_content.append('IC50')\n",
    "                continue\n",
    "            if i not in ['RosettaScore','TF','kd','IC50']:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "\n",
    "        for i in loss_content:\n",
    "            if i == 'RosettaScore':\n",
    "                total_loss += a * loss_Rossetta\n",
    "                continue\n",
    "            if i == 'TF':\n",
    "                total_loss += b * loss_TF\n",
    "                continue\n",
    "            if i == 'kd':\n",
    "                total_loss += c * loss_kd\n",
    "                continue\n",
    "            if i == 'IC50':\n",
    "                total_loss += d * loss_IC50\n",
    "                continue\n",
    "            if i not in ['RosettaScore','TF','kd','IC50']:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "        return total_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个版本的S2S_Loss已经可以实现对62个不同回归数值的基于不确定度的回归了，设置optimizer的时候需要注意一下。\n",
    "# 豆包给的初始化示例代码\n",
    "# model = ...  # 这里省略模型的定义\n",
    "# loss_fn = S2S_Loss(label_cut=[62, 1, 1, 1], train_target=['RosettaScore'], adaptive_args=[1.0, 0, 0, 0])\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()) + [loss_fn.log_var_rosetta], lr=0.001)\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 前向传播\n",
    "#     model_output = model(inputs)\n",
    "#     total_loss = loss_fn.total_loss(model_output, targets)\n",
    "    \n",
    "#     # 反向传播\n",
    "#     optimizer.zero_grad()\n",
    "#     total_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "\n",
    "class S2S_Loss():\n",
    "    '''\n",
    "    这个函数应该有的功能：\n",
    "        应对不同的cut形式\n",
    "        单独训练和加和训练两种模式\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 label_cut:list, #[62,1,1,1] # 前62个rosettascore和三个特殊变量\n",
    "                 train_target:list, # ['RosettaScore','kd','TF','IC50'],大小写区分清楚，只有kd小写\n",
    "                 adaptive_args:list,# 这玩意必须随着train_target调整啊，是一个必须调整的参数,长度必须为4，因为我实在是懒得接着写if了。eg：[0.3,0.3,0,0.3]\n",
    "                 ):\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.adaptive_args = adaptive_args\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        # 为RosettaScore的62个维度引入独立的不确定性参数\n",
    "        self.log_var_rosetta = nn.Parameter(torch.zeros(label_cut[0]))\n",
    "\n",
    "    def cut(self, model_output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        根据 label_cut 对模型输出进行裁剪\n",
    "        \"\"\"\n",
    "        cut_outputs = []\n",
    "        start = 0\n",
    "        for cut in self.label_cut:\n",
    "            end = start + cut\n",
    "            cut_outputs.append(model_output[:, start:end])\n",
    "            start = end\n",
    "        # cut_output = [[batch_size, 62], [batch_size, 1], [batch_size, 1], [batch_size, 1]] (if label_cut = [62, 1, 1, 1])\n",
    "        return cut_outputs\n",
    "\n",
    "    def MSE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算 MSE 损失\n",
    "        \"\"\"\n",
    "        return self.mse_loss_fn(output, target)\n",
    "\n",
    "    def BCE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算二元交叉熵损失\n",
    "        \"\"\"\n",
    "        return self.bce_loss_fn(output, target)\n",
    "\n",
    "    def uncertainty_mse_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算引入不确定性的MSE损失\n",
    "        \"\"\"\n",
    "        precision = torch.exp(-self.log_var_rosetta)\n",
    "        # 分别计算每个维度的MSE损失\n",
    "        mse_losses = torch.mean((output - target) ** 2, dim=0)\n",
    "        # 引入不确定性参数\n",
    "        losses = precision * mse_losses + self.log_var_rosetta\n",
    "        # 对所有维度的损失求和\n",
    "        total_loss = torch.sum(losses)\n",
    "        return total_loss\n",
    "\n",
    "    def total_loss(self, model_output, targets):\n",
    "        \"\"\"\n",
    "        根据 train_mode 计算总损失\n",
    "        \"\"\"\n",
    "        cut_outputs = self.cut(model_output)\n",
    "        loss_Rosetta = 0\n",
    "        loss_TF = 0\n",
    "        loss_kd = 0\n",
    "        loss_IC50 = 0\n",
    "        loss_content = []\n",
    "        total_loss = 0\n",
    "        a,b,c,d = self.adaptive_args\n",
    "        \n",
    "        for i in self.train_target:\n",
    "            if i == 'RosettaScore':\n",
    "                loss_Rosetta = self.uncertainty_mse_loss(cut_outputs[0], targets[0])\n",
    "                loss_content.append('RosettaScore')\n",
    "                continue\n",
    "            if i == 'TF':\n",
    "                loss_TF = self.BCE_loss(cut_outputs[1], targets[1])\n",
    "                loss_content.append('TF')\n",
    "                continue\n",
    "            if i == 'kd':\n",
    "                loss_kd = self.MSE_loss(cut_outputs[2], targets[2])\n",
    "                loss_content.append('kd')\n",
    "                continue\n",
    "            if i == 'IC50':\n",
    "                loss_IC50 = self.BCE_loss(cut_outputs[3], targets[3])\n",
    "                loss_content.append('IC50')\n",
    "                continue\n",
    "            if i not in ['RosettaScore','TF','kd','IC50']:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "\n",
    "        for i in loss_content:\n",
    "            if i == 'RosettaScore':\n",
    "                total_loss += a * loss_Rosetta\n",
    "                continue\n",
    "            if i == 'TF':\n",
    "                total_loss += b * loss_TF\n",
    "                continue\n",
    "            if i == 'kd':\n",
    "                total_loss += c * loss_kd\n",
    "                continue\n",
    "            if i == 'IC50':\n",
    "                total_loss += d * loss_IC50\n",
    "                continue\n",
    "            if i not in ['RosettaScore','TF','kd','IC50']:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class S2S_Loss():\n",
    "    '''\n",
    "    这个函数应该有的功能：\n",
    "        应对不同的cut形式\n",
    "        单独训练和加和训练两种模式\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 label_cut:list, #[62,1,1,1] # 前62个rosettascore和三个特殊变量\n",
    "                 train_target:list, # ['RosettaScore','kd','TF','IC50'],大小写区分清楚，只有kd小写\n",
    "                 adaptive_args:list,# 这玩意必须随着train_target调整啊，是一个必须调整的参数,长度必须为4，因为我实在是懒得接着写if了。eg：[0.3,0.3,0,0.3]\n",
    "                 alpha: float = 1.5,  # GradNorm 中的比例系数\n",
    "                 ):\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.adaptive_args = adaptive_args\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        # 为RosettaScore的62个维度引入独立的不确定性参数\n",
    "        self.log_var_rosetta = nn.Parameter(torch.zeros(label_cut[0]))\n",
    "        self.alpha = alpha\n",
    "        # 初始化每个任务的权重，初始值为 1，并设置为可训练参数\n",
    "        self.weights = nn.Parameter(torch.ones(len(train_target), requires_grad=True))\n",
    "        # 定义任务优先级\n",
    "        self.priority = {'IC50': 4, 'kd': 3, 'TF': 2, 'RosettaScore': 1}\n",
    "\n",
    "    def cut(self, model_output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        根据 label_cut 对模型输出进行裁剪\n",
    "        \"\"\"\n",
    "        cut_outputs = []\n",
    "        start = 0\n",
    "        for cut in self.label_cut:\n",
    "            end = start + cut\n",
    "            cut_outputs.append(model_output[:, start:end])\n",
    "            start = end\n",
    "        # cut_output = [[batch_size, 62], [batch_size, 1], [batch_size, 1], [batch_size, 1]] (if label_cut = [62, 1, 1, 1])\n",
    "        return cut_outputs\n",
    "\n",
    "    def MSE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算 MSE 损失\n",
    "        \"\"\"\n",
    "        return self.mse_loss_fn(output, target)\n",
    "\n",
    "    def BCE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算二元交叉熵损失\n",
    "        \"\"\"\n",
    "        return self.bce_loss_fn(output, target)\n",
    "\n",
    "    def uncertainty_mse_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算引入不确定性的MSE损失\n",
    "        \"\"\"\n",
    "        precision = torch.exp(-self.log_var_rosetta)\n",
    "        # 分别计算每个维度的MSE损失\n",
    "        mse_losses = torch.mean((output - target) ** 2, dim=0)\n",
    "        # 引入不确定性参数\n",
    "        losses = precision * mse_losses + self.log_var_rosetta\n",
    "        # 对所有维度的损失求和\n",
    "        total_loss = torch.sum(losses)\n",
    "        return total_loss\n",
    "\n",
    "    def total_loss(self, model_output, targets, model):\n",
    "        \"\"\"\n",
    "        根据 train_mode 计算总损失\n",
    "        \"\"\"\n",
    "        cut_outputs = self.cut(model_output)\n",
    "        losses = []\n",
    "        loss_content = []\n",
    "        task_index = {}\n",
    "\n",
    "        for i, target_name in enumerate(self.train_target):\n",
    "            if target_name == 'RosettaScore':\n",
    "                loss = self.uncertainty_mse_loss(cut_outputs[0], targets[0])\n",
    "                loss_content.append('RosettaScore')\n",
    "            elif target_name == 'TF':\n",
    "                loss = self.BCE_loss(cut_outputs[1], targets[1])\n",
    "                loss_content.append('TF')\n",
    "            elif target_name == 'kd':\n",
    "                loss = self.MSE_loss(cut_outputs[2], targets[2])\n",
    "                loss_content.append('kd')\n",
    "            elif target_name == 'IC50':\n",
    "                loss = self.BCE_loss(cut_outputs[3], targets[3])\n",
    "                loss_content.append('IC50')\n",
    "            else:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "            losses.append(loss)\n",
    "            task_index[target_name] = i\n",
    "\n",
    "        # 计算每个损失项的梯度范数\n",
    "        grads = []\n",
    "        for loss in losses:\n",
    "            # 计算当前损失项对模型参数的梯度\n",
    "            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "            # 计算梯度的范数\n",
    "            grad_norm = torch.norm(torch.stack([torch.norm(g) for g in grad]))\n",
    "            # 将梯度范数添加到列表中\n",
    "            grads.append(grad_norm)\n",
    "\n",
    "        # 计算初始梯度范数\n",
    "        initial_grad_norm = torch.stack(grads).mean()\n",
    "\n",
    "        # 计算每个损失项的梯度范数相对于初始梯度范数的比例\n",
    "        relative_grad_norms = [grad / initial_grad_norm for grad in grads]\n",
    "\n",
    "        # 计算梯度范数的平均值\n",
    "        avg_relative_grad_norm = sum(relative_grad_norms) / len(relative_grad_norms)\n",
    "\n",
    "        # 引入任务优先级调整梯度范数差异\n",
    "        priority_grad_norm_diff = []\n",
    "        for i, target_name in enumerate(self.train_target):\n",
    "            priority = self.priority[target_name]\n",
    "            grad_norm_diff = (relative_grad_norms[i] - avg_relative_grad_norm) ** 2\n",
    "            priority_grad_norm_diff.append(priority * grad_norm_diff)\n",
    "\n",
    "        # 计算 GradNorm 损失\n",
    "        gradnorm_loss = sum(priority_grad_norm_diff) * self.alpha\n",
    "\n",
    "        # 计算加权后的总损失\n",
    "        weighted_losses = [w * loss for w, loss in zip(self.weights, losses)]\n",
    "        total_loss = sum(weighted_losses) + gradnorm_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "# 示例初始化\n",
    "label_cut = [62, 1, 1, 1]\n",
    "train_target = ['RosettaScore', 'kd', 'TF', 'IC50']\n",
    "adaptive_args = [0.2, 0.2, 0.2, 0.4]\n",
    "alpha = 1.5\n",
    "\n",
    "# 创建损失函数实例\n",
    "loss_fn = S2S_Loss(label_cut, train_target, adaptive_args, alpha)\n",
    "\n",
    "# 假设的模型输出和目标\n",
    "model_output = torch.randn(32, sum(label_cut))\n",
    "targets = [torch.randn(32, 62), torch.randn(32, 1), torch.randn(32, 1), torch.randn(32, 1)]\n",
    "\n",
    "# 假设的模型\n",
    "model = nn.Linear(sum(label_cut), sum(label_cut))\n",
    "\n",
    "# 计算总损失\n",
    "total_loss = loss_fn.total_loss(model_output, targets, model)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 来个优化文本的版本\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class S2S_Loss():\n",
    "    '''\n",
    "    该类用于计算多任务学习的损失，支持不同的 cut 形式和单独训练、加和训练两种模式。\n",
    "\n",
    "    参数:\n",
    "        label_cut (list): 用于对模型输出进行裁剪的列表，例如 [62, 1, 1, 1] 表示前 62 个为 RosettaScore，后三个为特殊变量。\n",
    "        train_target (list): 训练目标列表，元素必须为 ['RosettaScore', 'kd', 'TF', 'IC50'] 中的一个，大小写区分。\n",
    "        adaptive_args (list): 自适应参数列表，长度应与 train_target 一致。\n",
    "        alpha (float): GradNorm 中的比例系数，默认为 1.5。\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 label_cut: list,\n",
    "                 train_target: list,\n",
    "                 adaptive_args: list,\n",
    "                 alpha: float = 1.5):\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.adaptive_args = adaptive_args\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        # 为 RosettaScore 的 62 个维度引入独立的不确定性参数\n",
    "        self.log_var_rosetta = nn.Parameter(torch.zeros(label_cut[0]))\n",
    "        self.alpha = alpha\n",
    "        # 初始化每个任务的权重，初始值为 1，并设置为可训练参数\n",
    "        self.weights = nn.Parameter(torch.ones(len(train_target), requires_grad=True))\n",
    "        # 定义任务优先级\n",
    "        self.priority = {'IC50': 4, 'kd': 3, 'TF': 2, 'RosettaScore': 1}\n",
    "\n",
    "    def cut(self, model_output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        根据 label_cut 对模型输出进行裁剪。\n",
    "\n",
    "        参数:\n",
    "            model_output (torch.Tensor): 模型的输出张量。\n",
    "\n",
    "        返回:\n",
    "            list: 裁剪后的输出列表，每个元素为一个张量。\n",
    "        \"\"\"\n",
    "        cut_outputs = []\n",
    "        start = 0\n",
    "        for cut in self.label_cut:\n",
    "            end = start + cut\n",
    "            cut_outputs.append(model_output[:, start:end])\n",
    "            start = end\n",
    "        return cut_outputs\n",
    "\n",
    "    def MSE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算 MSE 损失。\n",
    "\n",
    "        参数:\n",
    "            output (torch.Tensor): 模型的输出张量。\n",
    "            target (torch.Tensor): 目标张量。\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: MSE 损失值。\n",
    "        \"\"\"\n",
    "        return self.mse_loss_fn(output, target)\n",
    "\n",
    "    def BCE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算二元交叉熵损失。\n",
    "\n",
    "        参数:\n",
    "            output (torch.Tensor): 模型的输出张量。\n",
    "            target (torch.Tensor): 目标张量。\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: 二元交叉熵损失值。\n",
    "        \"\"\"\n",
    "        return self.bce_loss_fn(output, target)\n",
    "\n",
    "    def uncertainty_mse_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算引入不确定性的 MSE 损失。\n",
    "\n",
    "        参数:\n",
    "            output (torch.Tensor): 模型的输出张量。\n",
    "            target (torch.Tensor): 目标张量。\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: 引入不确定性的 MSE 损失值。\n",
    "        \"\"\"\n",
    "        precision = torch.exp(-self.log_var_rosetta)\n",
    "        # 分别计算每个维度的 MSE 损失\n",
    "        mse_losses = torch.mean((output - target) ** 2, dim=0)\n",
    "        # 引入不确定性参数\n",
    "        losses = precision * mse_losses + self.log_var_rosetta\n",
    "        # 对所有维度的损失求和\n",
    "        total_loss = torch.sum(losses)\n",
    "        return total_loss\n",
    "\n",
    "    def _get_loss(self, target_name, cut_outputs, targets):\n",
    "        \"\"\"\n",
    "        根据训练目标名称计算相应的损失。\n",
    "\n",
    "        参数:\n",
    "            target_name (str): 训练目标名称，必须为 ['RosettaScore', 'kd', 'TF', 'IC50'] 中的一个。\n",
    "            cut_outputs (list): 裁剪后的输出列表。\n",
    "            targets (list): 目标列表。\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: 相应的损失值。\n",
    "        \"\"\"\n",
    "        if target_name == 'RosettaScore':\n",
    "            return self.uncertainty_mse_loss(cut_outputs[0], targets[0])\n",
    "        elif target_name == 'TF':\n",
    "            return self.BCE_loss(cut_outputs[1], targets[1])\n",
    "        elif target_name == 'kd':\n",
    "            return self.MSE_loss(cut_outputs[2], targets[2])\n",
    "        elif target_name == 'IC50':\n",
    "            return self.BCE_loss(cut_outputs[3], targets[3])\n",
    "        else:\n",
    "            raise ValueError(\"train_target must be one of ['RosettaScore', 'TF', 'kd', 'IC50']\")\n",
    "\n",
    "    def total_loss(self, model_output, targets, model):\n",
    "        \"\"\"\n",
    "        根据 train_mode 计算总损失。\n",
    "\n",
    "        参数:\n",
    "            model_output (torch.Tensor): 模型的输出张量。\n",
    "            targets (list): 目标列表。\n",
    "            model (torch.nn.Module): 模型对象。\n",
    "\n",
    "        返回:\n",
    "            torch.Tensor: 总损失值。\n",
    "        \"\"\"\n",
    "        cut_outputs = self.cut(model_output)\n",
    "        losses = []\n",
    "        loss_content = []\n",
    "        task_index = {}\n",
    "\n",
    "        for i, target_name in enumerate(self.train_target):\n",
    "            loss = self._get_loss(target_name, cut_outputs, targets)\n",
    "            loss_content.append(target_name)\n",
    "            losses.append(loss)\n",
    "            task_index[target_name] = i\n",
    "\n",
    "        # 计算每个损失项的梯度范数\n",
    "        grads = []\n",
    "        for loss in losses:\n",
    "            # 计算当前损失项对模型参数的梯度\n",
    "            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "            # 计算梯度的范数\n",
    "            grad_norm = torch.norm(torch.stack([torch.norm(g) for g in grad]))\n",
    "            # 将梯度范数添加到列表中\n",
    "            grads.append(grad_norm)\n",
    "\n",
    "        # 计算初始梯度范数\n",
    "        initial_grad_norm = torch.stack(grads).mean()\n",
    "\n",
    "        # 计算每个损失项的梯度范数相对于初始梯度范数的比例\n",
    "        relative_grad_norms = [grad / initial_grad_norm for grad in grads]\n",
    "\n",
    "        # 计算梯度范数的平均值\n",
    "        avg_relative_grad_norm = sum(relative_grad_norms) / len(relative_grad_norms)\n",
    "\n",
    "        # 引入任务优先级调整梯度范数差异\n",
    "        priority_grad_norm_diff = []\n",
    "        for i, target_name in enumerate(self.train_target):\n",
    "            priority = self.priority[target_name]\n",
    "            grad_norm_diff = (relative_grad_norms[i] - avg_relative_grad_norm) ** 2\n",
    "            priority_grad_norm_diff.append(priority * grad_norm_diff)\n",
    "\n",
    "        # 计算 GradNorm 损失\n",
    "        gradnorm_loss = sum(priority_grad_norm_diff) * self.alpha\n",
    "\n",
    "        # 计算加权后的总损失\n",
    "        weighted_losses = [w * loss for w, loss in zip(self.weights, losses)]\n",
    "        total_loss = sum(weighted_losses) + gradnorm_loss\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个比较正统的gradnormloss的计算类\n",
    "class GradNormLoss(nn.Module):\n",
    "    def __init__(self, num_of_task, alpha=1.5):\n",
    "        super(GradNormLoss, self).__init__()\n",
    "        self.num_of_task = num_of_task\n",
    "        self.alpha = alpha\n",
    "        self.w = nn.Parameter(torch.ones(num_of_task, dtype=torch.float))\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.L_0 = None\n",
    "\n",
    "    # standard forward pass\n",
    "    def forward(self, L_t: torch.Tensor):\n",
    "        # initialize the initial loss `Li_0`\n",
    "        if self.L_0 is None:\n",
    "            self.L_0 = L_t.detach() # detach\n",
    "        # compute the weighted loss w_i(t) * L_i(t)\n",
    "        self.L_t = L_t\n",
    "        self.wL_t = L_t * self.w\n",
    "        # the reduced weighted loss\n",
    "        self.total_loss = self.wL_t.sum()\n",
    "        return self.total_loss\n",
    "\n",
    "    # additional forward & backward pass\n",
    "    def additional_forward_and_backward(self, grad_norm_weights: nn.Module, \n",
    "            optimizer: optim.Optimizer):\n",
    "        # do `optimizer.zero_grad()` outside\n",
    "        self.total_loss.backward(retain_graph=True)\n",
    "        # in standard backward pass, `w` does not require grad\n",
    "        self.w.grad.data = self.w.grad.data * 0.0\n",
    "\n",
    "        self.GW_t = []\n",
    "        for i in range(self.num_of_task):\n",
    "            # get the gradient of this task loss with respect to the shared parameters\n",
    "            GiW_t = torch.autograd.grad(\n",
    "                self.L_t[i], \n",
    "                grad_norm_weights.parameters(), # 这个参数的功能是指定哪一些层的参数会被计算\n",
    "                retain_graph=True, \n",
    "                create_graph=True\n",
    "                )\n",
    "            # compute the norm\n",
    "            self.GW_t.append(torch.norm(GiW_t[0] * self.w[i]))\n",
    "        self.GW_t = torch.stack(self.GW_t) # do not detatch\n",
    "        self.bar_GW_t = self.GW_t.detach().mean()\n",
    "        self.tilde_L_t = (self.L_t / self.L_0).detach()\n",
    "        self.r_t = self.tilde_L_t / self.tilde_L_t.mean()\n",
    "        grad_loss = self.l1_loss(self.GW_t, self.bar_GW_t * (self.r_t ** self.alpha))\n",
    "        self.w.grad = torch.autograd.grad(grad_loss, self.w)[0]\n",
    "        optimizer.step()\n",
    "\n",
    "        self.GW_ti, self.bar_GW_t, self.tilde_L_t, \n",
    "            self.r_t, self.L_t, self.wL_t = None, None, None, None, None, None\n",
    "        # re-norm\n",
    "        self.w.data = self.w.data / self.w.data.sum() * self.num_of_task\n",
    "\n",
    "# This is AN interface.\n",
    "class GradNormModel:\n",
    "    def get_grad_norm_weights(self) -> nn.Module:\n",
    "        raise NotImplementedError(\n",
    "            \"Please implement the method `get_grad_norm_weights`\")\n",
    "\n",
    "# # 定义一个简单的多任务模型，包含共享参数模块\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        # 定义共享参数模块\n",
    "        self.shared_layer = nn.Linear(10, 20)\n",
    "        # 定义任务特定的输出层\n",
    "        self.task1_output = nn.Linear(20, 1)\n",
    "        self.task2_output = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过共享层\n",
    "        shared_output = self.shared_layer(x)\n",
    "        # 分别通过任务特定的输出层\n",
    "        task1_output = self.task1_output(shared_output)\n",
    "        task2_output = self.task2_output(shared_output)\n",
    "        return task1_output, task2_output\n",
    "\n",
    "    def get_grad_norm_weights(self):\n",
    "        return self.shared_layer\n",
    "\n",
    "\n",
    "# 调用的示例代码\n",
    "# # 超参数设置\n",
    "# num_epochs = 10\n",
    "# batch_size = 32\n",
    "# learning_rate = 0.01\n",
    "# num_of_task = 2\n",
    "\n",
    "# # 实例化模型\n",
    "# model = MultiTaskModel()\n",
    "\n",
    "# # 实例化 GradNormLoss\n",
    "# grad_norm_loss = GradNormLoss(num_of_task)\n",
    "\n",
    "# # 定义优化器\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # 模拟数据集\n",
    "# num_samples = 1000\n",
    "# x = torch.randn(num_samples, 10)\n",
    "# target1 = torch.randn(num_samples, 1)\n",
    "# target2 = torch.randn(num_samples, 1)\n",
    "# dataset = torch.utils.data.TensorDataset(x, target1, target2)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_total_loss = 0.0\n",
    "#     for batch_x, batch_target1, batch_target2 in dataloader:\n",
    "#         # 前向传播\n",
    "#         task1_output, task2_output = model(batch_x)\n",
    "\n",
    "#         # 计算每个任务的损失\n",
    "#         loss1 = nn.MSELoss()(task1_output, batch_target1)\n",
    "#         loss2 = nn.MSELoss()(task2_output, batch_target2)\n",
    "#         L_t = torch.stack([loss1, loss2])\n",
    "\n",
    "#         # 计算总损失\n",
    "#         total_loss = grad_norm_loss(L_t)\n",
    "\n",
    "#         # 清空梯度\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # 额外的前向和反向传播，调整任务损失权重\n",
    "#         grad_norm_loss.additional_forward_and_backward(\n",
    "#             grad_norm_weights=model.get_grad_norm_weights(),\n",
    "#             optimizer=optimizer\n",
    "#         )\n",
    "\n",
    "#         # 再次计算总损失相对于模型参数的梯度\n",
    "#         total_loss.backward()\n",
    "\n",
    "#         # 更新模型参数\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_total_loss += total_loss.item()\n",
    "\n",
    "#     # 打印每个 epoch 的平均总损失\n",
    "#     print(f'Epoch {epoch + 1}/{num_epochs}, Average Total Loss: {running_total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终版本，输出的应该是一个包含各个loss的list，后面使用自定义训练器进行训练（由于需要进行gradnorm）\n",
    "# 豆包给的初始化示例代码\n",
    "# model = ...  # 这里省略模型的定义\n",
    "# loss_fn = S2S_Loss(label_cut=[62, 1, 1, 1], train_target=['RosettaScore'], adaptive_args=[1.0, 0, 0, 0])\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()) + [loss_fn.log_var_rosetta], lr=0.001)\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# 训练循环\n",
    "# for epoch in range(num_epochs):\n",
    "#     # 前向传播\n",
    "#     model_output = model(inputs)\n",
    "#     total_loss = loss_fn.total_loss(model_output, targets)\n",
    "    \n",
    "#     # 反向传播\n",
    "#     optimizer.zero_grad()\n",
    "#     total_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "\n",
    "class S2S_Loss():\n",
    "    '''\n",
    "    这个函数应该有的功能：\n",
    "        应对不同的cut形式\n",
    "        单独训练和加和训练两种模式\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 label_cut:list, #[62,1,1,1] # 前62个rosettascore和三个特殊变量\n",
    "                 train_target:list, # ['RosettaScore','kd','TF','IC50'],大小写区分清楚，只有kd小写\n",
    "                 ):\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        # 为RosettaScore的62个维度引入独立的不确定性参数\n",
    "        self.log_var_rosetta = nn.Parameter(torch.zeros(label_cut[0]))\n",
    "\n",
    "    def cut(self, model_output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        根据 label_cut 对模型输出进行裁剪\n",
    "        \"\"\"\n",
    "        cut_outputs = []\n",
    "        start = 0\n",
    "        for cut in self.label_cut:\n",
    "            end = start + cut\n",
    "            cut_outputs.append(model_output[:, start:end])\n",
    "            start = end\n",
    "        # cut_output = [[batch_size, 62], [batch_size, 1], [batch_size, 1], [batch_size, 1]] (if label_cut = [62, 1, 1, 1])\n",
    "        return cut_outputs\n",
    "\n",
    "    def MSE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算 MSE 损失\n",
    "        \"\"\"\n",
    "        return self.mse_loss_fn(output, target)\n",
    "\n",
    "    def BCE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算二元交叉熵损失\n",
    "        \"\"\"\n",
    "        return self.bce_loss_fn(output, target)\n",
    "\n",
    "    def uncertainty_mse_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算引入不确定性的MSE损失\n",
    "        \"\"\"\n",
    "        precision = torch.exp(-self.log_var_rosetta)\n",
    "        # 分别计算每个维度的MSE损失\n",
    "        mse_losses = torch.mean((output - target) ** 2, dim=0)\n",
    "        # 引入不确定性参数\n",
    "        losses = precision * mse_losses + self.log_var_rosetta\n",
    "        # 对所有维度的损失求和\n",
    "        total_loss = torch.sum(losses)\n",
    "        return total_loss\n",
    "\n",
    "    def total_loss(self, model_output, targets):\n",
    "        \"\"\"\n",
    "        根据 train_mode 计算总损失\n",
    "        \"\"\"\n",
    "        cut_outputs = self.cut(model_output)\n",
    "        loss_Rosetta = 0\n",
    "        loss_TF = 0\n",
    "        loss_kd = 0\n",
    "        loss_IC50 = 0\n",
    "        loss_content = []\n",
    "        targets = self.cut(targets)\n",
    "        \n",
    "        for i in self.train_target:\n",
    "            if i == 'RosettaScore':\n",
    "                loss_Rosetta = self.uncertainty_mse_loss(cut_outputs[0], targets[0])\n",
    "                loss_content.append(loss_Rosetta)\n",
    "                del loss_Rosetta\n",
    "                continue\n",
    "            if i == 'TF':\n",
    "                loss_TF = self.BCE_loss(cut_outputs[1], targets[1])\n",
    "                loss_content.append(loss_TF)\n",
    "                del loss_TF\n",
    "                continue\n",
    "            if i == 'kd':\n",
    "                loss_kd = self.MSE_loss(cut_outputs[2], targets[2])\n",
    "                loss_content.append(loss_kd)\n",
    "                del loss_kd\n",
    "                continue\n",
    "            if i == 'IC50':\n",
    "                loss_IC50 = self.BCE_loss(cut_outputs[3], targets[3])\n",
    "                loss_content.append(loss_IC50)\n",
    "                del loss_IC50\n",
    "                continue\n",
    "            if i not in ['RosettaScore','TF','kd','IC50']:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "            \n",
    "        return loss_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/users/hcdai/AI-peptide/Seq2Score/Seq2Rosscore/ESMC-MLP/ESMC-attn/weight_of_attn_20250125/attn_baseline',\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/home/users/hcdai/AI-peptide/Seq2Score/Seq2Rosscore/ESMC-MLP/ESMC-attn/log_of_attn_20250125/attn_baseline',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # 启用混合精度训练\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch = None):\n",
    "        # print(\"Inputs:\", inputs['labels'].size)\n",
    "        labels = inputs.pop(\"labels\").to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        return (loss, outputs[1]) if return_outputs else loss\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 获取当前 epoch 的验证损失\n",
    "        eval_metrics = self.evaluate()\n",
    "        val_loss = eval_metrics[\"eval_loss\"]\n",
    "        # 根据验证损失调整学习率\n",
    "        self.lr_scheduler.step(val_loss)\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    # 重新划分训练集和验证集，验证集比例为 10%\n",
    "    train_df, valid_df = train_test_split(output_ESMC, test_size=0.1, random_state=epoch)\n",
    "    \n",
    "    # 重置索引，保证索引连续，避免出现KeyError\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = PeptideDataset(df = train_df,\n",
    "                                   score_columns = score_columns,\n",
    "                                   esmc_hidden_column = 'ESMC_Embedding_Path')\n",
    "\n",
    "    valid_dataset = PeptideDataset(df = valid_df,\n",
    "                                   score_columns = score_columns,\n",
    "                                   esmc_hidden_column = 'ESMC_Embedding_Path')\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=None,\n",
    "    )\n",
    "\n",
    "    # 数据加载器\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size,\n",
    "                                                   shuffle=True, pin_memory=True)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=training_args.per_device_eval_batch_size,\n",
    "                                                  shuffle=False, pin_memory=True)\n",
    "\n",
    "    # 使用加速器包装模型和数据加载器\n",
    "    model = accelerator.prepare(model)\n",
    "    train_dataloader, eval_dataloader = accelerator.prepare(train_dataloader, eval_dataloader)\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 保存模型\n",
    "    model_save_path = f'/home/users/hcdai/AI-peptide/Seq2Score/Seq2Rosscore/ESMC-MLP/ESMC-attn/checkpoint_of_attn_20250125/attn_baseline/attn_baseline_checkpoint_epoch_{epoch}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # 评估模型\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Epoch {epoch + 1} Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, model, args, train_dataset, eval_dataset, data_collator, label_cut, train_target, accelerator):\n",
    "        super().__init__(model, args, train_dataset, eval_dataset, data_collator)\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.accelerator = accelerator\n",
    "        self.s2s_loss = S2S_Loss(label_cut, train_target)\n",
    "        self.grad_norm_loss = GradNormLoss(num_of_task=len(train_target))\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        # 这里假设 optimizer 已经在外部定义\n",
    "        self.optimizer = optimizer\n",
    "        # 可以根据需要在这里定义学习率调度器\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").to(self.accelerator.device)\n",
    "        inputs = {k: v.to(self.accelerator.device) for k, v in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # 计算每个任务的损失\n",
    "        loss_content = self.s2s_loss.total_loss(outputs, labels)\n",
    "        loss_content = torch.stack(loss_content)\n",
    "\n",
    "        # 使用 GradNormLoss 计算加权损失\n",
    "        total_loss = self.grad_norm_loss(loss_content)\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # 清零梯度\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # 计算损失\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        # 反向传播\n",
    "        self.accelerator.backward(loss)\n",
    "\n",
    "        # 执行 GradNorm 的额外前向和反向传播步骤\n",
    "        grad_norm_weights = model.get_grad_norm_weights()\n",
    "        self.grad_norm_loss.additional_forward_and_backward(grad_norm_weights, self.optimizer)\n",
    "\n",
    "        # 更新模型参数\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach()\n",
    "\n",
    "\n",
    "# 以下是使用示例\n",
    "# 假设 model 已经定义\n",
    "model = ...\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/users/hcdai/AI-peptide/Seq2Score/Seq2Rosscore/ESMC-MLP/ESMC-attn/weight_of_attn_20250125/attn_baseline',\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='/home/users/hcdai/AI-peptide/Seq2Score/Seq2Rosscore/ESMC-MLP/ESMC-attn/log_of_attn_20250125/attn_baseline',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # 启用混合精度训练\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 假设 output_ESMC 和 score_columns 已经定义\n",
    "output_ESMC = ...\n",
    "score_columns = ...\n",
    "\n",
    "# 损失函数参数\n",
    "label_cut = [62, 1, 1, 1]\n",
    "train_target = ['RosettaScore', 'kd', 'TF', 'IC50']\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    # 重新划分训练集和验证集，验证集比例为 10%\n",
    "    train_df, valid_df = train_test_split(output_ESMC, test_size=0.1, random_state=epoch)\n",
    "    \n",
    "    # 重置索引，保证索引连续，避免出现KeyError\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    valid_df = valid_df.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = PeptideDataset(df = train_df,\n",
    "                                   score_columns = score_columns,\n",
    "                                   esmc_hidden_column = 'ESMC_Embedding_Path')\n",
    "\n",
    "    valid_dataset = PeptideDataset(df = valid_df,\n",
    "                                   score_columns = score_columns,\n",
    "                                   esmc_hidden_column = 'ESMC_Embedding_Path')\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=None,\n",
    "        label_cut=label_cut,\n",
    "        train_target=train_target,\n",
    "        accelerator=accelerator\n",
    "    )\n",
    "\n",
    "    # 数据加载器\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size,\n",
    "                                                   shuffle=True, pin_memory=True)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=training_args.per_device_eval_batch_size,\n",
    "                                                  shuffle=False, pin_memory=True)\n",
    "\n",
    "    # 使用加速器包装模型和数据加载器\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 保存模型\n",
    "    model_save_path = f'/home/users/hcdai/AI-peptide/Seq2Score/Seq2Rosscore/ESMC-MLP/ESMC-attn/checkpoint_of_attn_20250125/attn_baseline/attn_baseline_checkpoint_epoch_{epoch}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # 评估模型\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Epoch {epoch + 1} Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_984436/780412936.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def check_pt_files(folder_path):\n",
    "    # 遍历文件夹中的所有文件\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.pt'):\n",
    "                # 构建文件的完整路径\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # 加载.pt文件\n",
    "                    tensor = torch.load(file_path)\n",
    "                    # 检查张量的形状是否符合要求\n",
    "                    if tensor.shape != (36, 1, 256, 1152):\n",
    "                        print(f\"文件 {file_path} 的形状不符合 [1, 256, 1152]，实际形状为 {tensor.shape}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"加载文件 {file_path} 时出现错误: {e}\")\n",
    "\n",
    "# 指定要遍历的文件夹路径\n",
    "folder_path = '/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/dataset/ESMC_embedding/TF_ori_withpath/merge'  # 请替换为实际的文件夹路径\n",
    "check_pt_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len([\n",
    "        'scores_total_score', 'scores_dslf_fa13',\n",
    "        'scores_fa_atr', 'scores_fa_dun', 'scores_fa_elec',\n",
    "        'scores_fa_intra_rep', 'scores_fa_intra_sol_xover4', 'scores_fa_rep',\n",
    "        'scores_fa_sol', 'scores_hbond_bb_sc', 'scores_hbond_lr_bb',\n",
    "        'scores_hbond_sc', 'scores_hbond_sr_bb', \n",
    "        'scores_lk_ball_wtd', 'scores_omega', \n",
    "        'scores_p_aa_pp', 'scores_pro_close', 'scores_rama_prepro',\n",
    "        'scores_ref',\n",
    "        'pack_total_score', 'pack_complex_normalized', 'pack_dG_separated',\n",
    "        'pack_dG_separated/dSASAx100', 'pack_dSASA_hphobic',\n",
    "        'pack_dSASA_int', 'pack_dSASA_polar', 'pack_delta_unsatHbonds',\n",
    "        'pack_dslf_fa13', 'pack_fa_atr', 'pack_fa_dun', 'pack_fa_elec',\n",
    "        'pack_fa_intra_rep', 'pack_fa_intra_sol_xover4', 'pack_fa_rep',\n",
    "        'pack_fa_sol', 'pack_hbond_E_fraction', 'pack_hbond_bb_sc',\n",
    "        'pack_hbond_lr_bb', 'pack_hbond_sc', 'pack_hbond_sr_bb',\n",
    "        'pack_hbonds_int', 'pack_lk_ball_wtd', 'pack_nres_all',\n",
    "        'pack_nres_int', 'pack_omega', 'pack_p_aa_pp', 'pack_packstat',\n",
    "        'pack_per_residue_energy_int', 'pack_pro_close', 'pack_rama_prepro',\n",
    "        'pack_ref', 'pack_sc_value', 'pack_side1_normalized',\n",
    "        'pack_side1_score', 'pack_side2_normalized', 'pack_side2_score',\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mUnexpected end of JSON input. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "class S2S_Loss():\n",
    "    '''\n",
    "    这个函数应该有的功能：\n",
    "        应对不同的cut形式\n",
    "        单独训练和加和训练两种模式\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 label_cut:list, #[62,1,1,1] # 前62个rosettascore和三个特殊变量\n",
    "                 train_target:list, # ['RosettaScore','kd','TF','IC50'],大小写区分清楚，只有kd小写\n",
    "                 ):\n",
    "        self.label_cut = label_cut\n",
    "        self.train_target = train_target\n",
    "        self.mse_loss_fn = nn.MSELoss()\n",
    "        self.bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        # 为RosettaScore的62个维度引入独立的不确定性参数\n",
    "        self.log_var_rosetta = nn.Parameter(torch.tensor([-0.5]* 62))\n",
    "        \n",
    "        self.csv_file = open('/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/debug/20250217_why_so_big/2.csv', 'w', newline='')\n",
    "        self.csv_writer = csv.writer(self.csv_file)\n",
    "        self.csv_row = []\n",
    "\n",
    "    def cut(self, model_output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        根据 label_cut 对模型输出进行裁剪\n",
    "        \"\"\"\n",
    "        cut_outputs = []\n",
    "        start = 0\n",
    "        for cut in self.label_cut:\n",
    "            end = start + cut\n",
    "            cut_outputs.append(model_output[:, start:end])\n",
    "            start = end\n",
    "        # cut_output = [[batch_size, 62], [batch_size, 1], [batch_size, 1], [batch_size, 1]] (if label_cut = [62, 1, 1, 1])\n",
    "        return cut_outputs\n",
    "\n",
    "    def MSE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算 MSE 损失\n",
    "        \"\"\"\n",
    "        print(f'target{target},output{output},mse{self.mse_loss_fn(output, target)}')\n",
    "        return self.mse_loss_fn(output, target)\n",
    "\n",
    "    def BCE_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算二元交叉熵损失\n",
    "        \"\"\"\n",
    "        if output.shape[1] == 2:\n",
    "            # 这里假设 target 为 0 或 1，取对应位置的概率\n",
    "            probabilities = output.gather(1, target.long().unsqueeze(1)).squeeze(1)\n",
    "            # 计算 BCE 损失\n",
    "            return self.bce_loss_fn(probabilities, target.squeeze(1))\n",
    "        \n",
    "        return self.bce_loss_fn(output, target)\n",
    "\n",
    "    def uncertainty_mse_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        计算引入不确定性的MSE损失\n",
    "        \"\"\"\n",
    "        # log_var_rosetta = self.log_var_rosetta.to(output.device)\n",
    "        # precision = torch.exp(-log_var_rosetta)\n",
    "        # # 分别计算每个维度的MSE损失\n",
    "        # mse_losses = torch.mean((output - target) ** 2, dim=0)\n",
    "        # # 引入不确定性参数\n",
    "        # losses = precision * mse_losses + log_var_rosetta\n",
    "        # # 对所有维度的损失求和\n",
    "        # total_loss = torch.sum(losses)\n",
    "        log_var_rosetta = self.log_var_rosetta.to(output.device)\n",
    "        # 分别计算每个维度的MSE损失\n",
    "        # self.csv_row = []\n",
    "        mse_losses = (output - target) ** 2\n",
    "        \n",
    "        # 引入不确定性参数\n",
    "        loss = (mse_losses/(2 * log_var_rosetta.exp())+log_var_rosetta.exp()/2).sum()\n",
    "        # loss.backward()\n",
    "        # self.csv_row.extend(mse_losses)\n",
    "        # self.csv_row.extend(mse_losses/(2 * log_var_rosetta.exp())+log_var_rosetta.exp()/2)\n",
    "        # print(f'mse_losses:{mse_losses},loss:{loss},loss_func:{mse_losses/(2 * log_var_rosetta.exp())+log_var_rosetta.exp()/2},model_output:{output},target:{target}')\n",
    "        \n",
    "        self.csv_row.append(f'mse_losses:{mse_losses},loss:{loss},loss_func:{mse_losses/(2 * log_var_rosetta.exp())+log_var_rosetta.exp()/2},model_output:{output},target:{target}')\n",
    "        self.csv_writer.writerow(self.csv_row)\n",
    "        self.csv_row = []\n",
    "        return loss\n",
    "\n",
    "    def total_loss(self, model_output, targets):\n",
    "        \"\"\"\n",
    "        根据 train_mode 计算总损失\n",
    "        \"\"\"\n",
    "        cut_outputs = self.cut(model_output)\n",
    "        loss_Rosetta = 0\n",
    "        loss_TF = 0\n",
    "        loss_kd = 0\n",
    "        loss_IC50 = 0\n",
    "        loss_content = []\n",
    "        targets = self.cut(targets)\n",
    "        \n",
    "        for i in self.train_target:\n",
    "            if i == 'RosettaScore':\n",
    "                loss_Rosetta = self.MSE_loss(cut_outputs[0], targets[0])\n",
    "                loss_content.append(loss_Rosetta)\n",
    "                del loss_Rosetta\n",
    "                continue\n",
    "            if i == 'TF':\n",
    "                loss_TF = self.BCE_loss(cut_outputs[1], targets[1])\n",
    "                loss_content.append(loss_TF)\n",
    "                del loss_TF\n",
    "                continue\n",
    "            if i == 'kd':\n",
    "                loss_kd = self.MSE_loss(cut_outputs[2], targets[2])\n",
    "                loss_content.append(loss_kd)\n",
    "                del loss_kd\n",
    "                continue\n",
    "            if i == 'IC50':\n",
    "                loss_IC50 = self.BCE_loss(cut_outputs[3], targets[3])\n",
    "                loss_content.append(loss_IC50)\n",
    "                del loss_IC50\n",
    "                continue\n",
    "            if i not in ['RosettaScore','TF','kd','IC50']:\n",
    "                raise ValueError(\"train_target must be one of ['RosettaScore','TF','kd','IC50']\")\n",
    "        \n",
    "        return loss_content\n",
    "    \n",
    "    database_path = '/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/trying/20250217_loss_too_big/dataset/generated_EK_withpath_train_try_deleted.csv'\n",
    "    label_cut = [56]\n",
    "    train_target = ['RosettaScore']\n",
    "    score_columns = [\n",
    "        'scores_total_score', 'scores_dslf_fa13',\n",
    "        'scores_fa_atr', 'scores_fa_dun', 'scores_fa_elec',\n",
    "        'scores_fa_intra_rep', 'scores_fa_intra_sol_xover4', 'scores_fa_rep',\n",
    "        'scores_fa_sol', 'scores_hbond_bb_sc', 'scores_hbond_lr_bb',\n",
    "        'scores_hbond_sc', 'scores_hbond_sr_bb', \n",
    "        'scores_lk_ball_wtd', 'scores_omega', \n",
    "        'scores_p_aa_pp', 'scores_pro_close', 'scores_rama_prepro',\n",
    "        'scores_ref',\n",
    "        'pack_total_score', 'pack_complex_normalized', 'pack_dG_separated',\n",
    "        'pack_dG_separated/dSASAx100', 'pack_dSASA_hphobic',\n",
    "        'pack_dSASA_int', 'pack_dSASA_polar', 'pack_delta_unsatHbonds',\n",
    "        'pack_dslf_fa13', 'pack_fa_atr', 'pack_fa_dun', 'pack_fa_elec',\n",
    "        'pack_fa_intra_rep', 'pack_fa_intra_sol_xover4', 'pack_fa_rep',\n",
    "        'pack_fa_sol', 'pack_hbond_E_fraction', 'pack_hbond_bb_sc',\n",
    "        'pack_hbond_lr_bb', 'pack_hbond_sc', 'pack_hbond_sr_bb',\n",
    "        'pack_hbonds_int', 'pack_lk_ball_wtd', 'pack_nres_all',\n",
    "        'pack_nres_int', 'pack_omega', 'pack_p_aa_pp', 'pack_packstat',\n",
    "        'pack_per_residue_energy_int', 'pack_pro_close', 'pack_rama_prepro',\n",
    "        'pack_ref', 'pack_sc_value', 'pack_side1_normalized',\n",
    "        'pack_side1_score', 'pack_side2_normalized', 'pack_side2_score',\n",
    "        ]\n",
    "    s2s_loss = S2S_Loss(label_cut, train_target)\n",
    "    database = pd.read_csv(database_path)\n",
    "    \n",
    "    train_df, valid_df = train_test_split(database, test_size=0.1, random_state=42)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    valid_df = valid_df.reset_index(drop=True)\n",
    "    \n",
    "    scores = train_df.loc[1, score_columns].values\n",
    "    scores = scores.astype(float)\n",
    "    scores = torch.tensor(scores, dtype=torch.float).to('cpu')\n",
    "    \n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import EsmTokenizer, EsmModel, Trainer, TrainingArguments\n",
    "from accelerate import Accelerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESMC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
