{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': {'workspace': '/home/users/hcdai/AI-peptide/ChaiRosettaScore', 'chai-1': {'ChaiRunPython': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/ChaiRunPython.sh', 'home': '/home/users/hcdai/AI-peptide/Chai-1/chai-lab', 'chaiworkdir': '/home/users/hcdai/AI-peptide/Chai-1/chai-lab/RMSD_Time-test', 'chai_python_executable': '/home/users/hcdai/miniconda3/envs/Chai-1/bin/python'}, 'rosetta': {'score_executable': '/home/users/hcdai/AI-peptide/rosetta_interface_analysis/rosetta.binary.ubuntu.release-371/main/source/bin/score_jd2.static.linuxgccrelease', 'InterfaceAnalyzer': '/home/users/hcdai/AI-peptide/rosetta_interface_analysis/rosetta.binary.ubuntu.release-371/main/source/bin/InterfaceAnalyzer.static.linuxgccrelease', 'pack_input_options': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/setting_options.ini'}, 'ligand_database': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/ligand', 'receptor_database': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/receptor', 'temp': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/temp', 'output': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/output', 'obabel': '/home/users/hcdai/miniconda3/envs/Chai-1/bin/obabel', 'output_csv': '/home/users/hcdai/AI-peptide/ChaiRosettaScore/output_result.csv'}, 'parameter': {'receptor_chain_num': 3}}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import ast\n",
    "# 读取配置文件\n",
    "with open('/home/users/hcdai/AI-peptide/RunRosetta/config_chai.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 打印配置文件内容\n",
    "print(config)\n",
    "\n",
    "path = config['path']\n",
    "parameter = config['parameter']\n",
    "# # 设定工作区路径\n",
    "# os.chdir(path[\"workspace\"])\n",
    "\n",
    "# getcwd = os.getcwd()\n",
    "# print(\"Current working directory: \", getcwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ligand_processor(csv_path):\n",
    "    # 定义一个空字典来存储结果\n",
    "    ligand_dict = {}\n",
    "    \n",
    "    # 打开CSV文件\n",
    "    with open(csv_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        # 创建CSV读取器\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        \n",
    "        # 遍历CSV文件的每一行\n",
    "        for row in csv_reader:\n",
    "            # 获取ligand_name的值作为键\n",
    "            ligand_name = row['ligand_name']\n",
    "            \n",
    "            # 获取ligand_sequence的值，并尝试将其从字符串解析为字典\n",
    "            try:\n",
    "                ligand_sequence = ast.literal_eval(row['ligand_sequence'])\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing ligand_sequence for {ligand_name}: {e}\")\n",
    "                continue  # 或者可以根据需要处理错误\n",
    "            \n",
    "            # 将解析后的ligand_sequence存入字典中\n",
    "            ligand_dict[ligand_name] = ligand_sequence\n",
    "    \n",
    "    return ligand_dict\n",
    "# # 以下代码为测试该函数用\n",
    "# file_path = '/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/dataset/standard_csv_withpath/TF_ori_withpath.csv'  # 替换为你的CSV文件路径\n",
    "# result_dict = ligand_processor(file_path)\n",
    " \n",
    "# # 打印结果\n",
    "# if result_dict is not None:\n",
    "#     print(result_dict)\n",
    "# else:\n",
    "#     print(\"No ligand found in the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receptor_processor(csv_path):\n",
    "    # 定义一个空字典来存储结果\n",
    "    receptor_dict = {}\n",
    "    \n",
    "    # 打开CSV文件\n",
    "    with open(csv_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        # 创建CSV读取器\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        \n",
    "        # 遍历CSV文件的每一行\n",
    "        for row in csv_reader:\n",
    "            # 获取receptor_name的值作为键\n",
    "            receptor_name = row['receptor_name']\n",
    "            \n",
    "            # 获取receptor的值，并尝试将其从字符串解析为字典\n",
    "            try:\n",
    "                receptor_sequence = ast.literal_eval(row['receptor_sequence'])\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing receptor_sequence for {receptor_name}: {e}\")\n",
    "                continue  # 或者可以根据需要处理错误\n",
    "            \n",
    "            # 将解析后的ligand_sequence存入字典中\n",
    "            receptor_dict[receptor_name] = receptor_sequence\n",
    "    \n",
    "    return receptor_dict\n",
    "# # 以下代码为测试该函数用\n",
    "# file_path = '/home/users/hcdai/AI-peptide/Seq2Score/Seq2Score_1.0/dataset/standard_csv_withpath/TF_ori_withpath.csv'  # 替换为你的CSV文件路径\n",
    "# result_dict = receptor_processor(file_path)\n",
    " \n",
    "# # 打印结果\n",
    "# if result_dict is not None:\n",
    "#     print(result_dict)\n",
    "# else:\n",
    "#     print(\"No receptor found in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_for_protein(protein_name, ligand_chains, receptor_chains, output_dir):\n",
    "    \"\"\"\n",
    "    为单个蛋白质写入FASTA文件，包含配体和受体链信息。\n",
    " \n",
    "    参数:\n",
    "    protein_name (str): 蛋白质的名称。\n",
    "    ligand_chains (dict): 配体链名称到序列的映射。\n",
    "    receptor_chains (dict): 受体链名称到序列的映射。\n",
    "    output_dir (str): 输出FASTA文件的目录。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    fasta_file_path = os.path.join(output_dir, f\"{protein_name}.fasta\")\n",
    "    \n",
    "    with open(fasta_file_path, 'w') as f:\n",
    "        # 写入配体信息\n",
    "        for chain_name, sequence in ligand_chains.items():\n",
    "            f.write(f'>protein|name={chain_name}\\n{sequence}\\n')\n",
    "        \n",
    "        # 写入受体信息\n",
    "        # 可选：为受体链名称添加前缀或后缀以区分它们\n",
    "        for chain_name, sequence in receptor_chains.items():\n",
    "            receptor_chain_name = f'_R_{chain_name}'  # 添加前缀 '_R' 以区分受体链\n",
    "            f.write(f'>protein|name={receptor_chain_name}\\n{sequence}\\n')\n",
    " \n",
    "def write_fasta_files_from_dicts(ligand_dict, receptor_dict, output_dir):\n",
    "    \"\"\"\n",
    "    从配体和受体词典中为每个蛋白质写入FASTA文件。\n",
    " \n",
    "    参数:\n",
    "    ligand_dict (dict): 配体信息的词典，键为蛋白质名称，值为链名称到序列的映射。\n",
    "    receptor_dict (dict): 受体信息的词典，键为蛋白质名称，值为链名称到序列的映射。\n",
    "    output_dir (str): 输出FASTA文件的目录。\n",
    "    \"\"\"\n",
    "    for protein_name in ligand_dict:\n",
    "        if protein_name in receptor_dict:\n",
    "            write_fasta_for_protein(protein_name, ligand_dict[protein_name], receptor_dict[protein_name], output_dir)\n",
    "        else:\n",
    "            print(f\"Warning: No receptor information found for protein {protein_name}\")\n",
    " \n",
    "# 示例词典\n",
    "ligand_example = {\n",
    "    'protein1': {'chain1': 'ATGCGTACGT', 'chain2': 'CGTACGTAGC'},\n",
    "    'protein2': {'chainA': 'GCTAGCTAGC', 'chainB': 'AGCTAGCTAG'}\n",
    "}\n",
    " \n",
    "receptor_example = {\n",
    "    'protein1': {'chainX': 'TAGCTAGCTA', 'chainY': 'GCTAGCTAGC'},\n",
    "    'protein2': {'chain1': 'CGTAGCTAGC', 'chain2': 'AGCTAGCTCG'}  # 注意这里chain2与ligand_example中的chain2不是同一个链，但名称相同\n",
    "}\n",
    " \n",
    "# 输出目录\n",
    "output_directory = '/home/users/hcdai/AI-peptide/RunRosetta/test_output_fasta'\n",
    " \n",
    "# 写入FASTA文件\n",
    "write_fasta_files_from_dicts(ligand_example, receptor_example, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据库的序列信息写入fasta文件\n",
    "csv_path=\"/home/users/hcdai/AI-peptide/RunRosetta/TF_ori_withpath.csv\"\n",
    "ligand_dict=ligand_processor(csv_path)\n",
    "receptor_dict=receptor_processor(csv_path)\n",
    "output_directory = '/home/users/hcdai/AI-peptide/RunRosetta/test_output_fasta'\n",
    "write_fasta_files_from_dicts(ligand_dict, receptor_dict, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3258237063.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    output_dir=\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def target_sequence_generator(temp_dir=path[\"temp\"],chaiworkdir=path[\"chaiworkdir\"]):\n",
    "    \n",
    "    output_dir=Path(os.path.join(temp_dir, \"cif\"))\n",
    "    # 目标序列文件\n",
    "    target_fasta_file = Path(\"/home/users/hcdai/AI-peptide/RunRosetta/test_output_fasta\")\n",
    "    # 目标脚本\n",
    "    script = f'''from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from chai_lab.chai1 import run_inference\n",
    "output_dir=Path(\"{output_dir}\")\n",
    "candidates = run_inference(\n",
    "    fasta_file=Path(\"{target_fasta_file}\"),\n",
    "    output_dir=output_dir,\n",
    "    # 'default' setup\n",
    "    num_trunk_recycles=3,\n",
    "    num_diffn_timesteps=200,\n",
    "    seed=42,\n",
    "    device=torch.device(\"cuda:0\"),\n",
    "    use_esm_embeddings=True,\n",
    ")\n",
    "cif_paths = candidates.cif_paths\n",
    "scores = [rd.aggregate_score for rd in candidates.ranking_data]\n",
    "# Load pTM, ipTM, pLDDTs and clash scores for sample 2\n",
    "scores = np.load(output_dir.joinpath(\"scores.model_idx_2.npz\"))'''  \n",
    "    \n",
    "    # 将目标脚本写入chaiworkdir下的run.py文件\n",
    "    run_file = Path(os.path.join(chaiworkdir, \"run.py\"))\n",
    "    with open(run_file, 'w') as f:\n",
    "        f.write(script)\n",
    "\n",
    "    # 成功写入脚本文件\n",
    "    print(f\"Chai script generated at {run_file}.\")\n",
    "\n",
    "    return run_file, output_dir, target_fasta_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义运行chai脚本函数\n",
    "def run_chai_script(chai_python_path,run_file):\n",
    "    # # 使用subprocess运行chai脚本\n",
    "    # run_file = str(run_file)\n",
    "    # command = [chai_python_path, run_file]\n",
    "    # print(\"command:\", command)    \n",
    "    # process = subprocess.run(command,shell=True,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # # 打印输出和错误信息\n",
    "    # print(\"输出:\", process.stdout.decode())\n",
    "    # print(\"错误:\", process.stderr.decode())\n",
    "\n",
    "    # 使用os.system运行chai脚本\n",
    "    run_file = str(run_file)\n",
    "    command = f\"{chai_python_path} {run_file}\"\n",
    "    os.system(command)\n",
    "\n",
    "    return True\n",
    "\n",
    "# 定义运行bash脚本函数\n",
    "def run_bash_script(bash_script_path):\n",
    "    # 使用subprocess运行bash脚本\n",
    "    bash_script_path = str(bash_script_path)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cif_to_pdb(cif_file_path,pdb_file_path):\n",
    "    # 读取cif文件转换为pdb文件\n",
    "    if not os.path.exists(pdb_file_path):\n",
    "        os.makedirs(pdb_file_path)\n",
    "    # 定义输入和输出目录\n",
    "    input_cif_directory = cif_file_path  \n",
    "    output_pdb_directory = pdb_file_path  \n",
    "\n",
    "    # 获取输入目录中所有CIF文件的列表并进行排序\n",
    "    cif_files = [f for f in os.listdir(input_cif_directory) if f.endswith(\".cif\")]\n",
    "    cif_files.sort()  # 按字母顺序排序\n",
    "\n",
    "    # 使用tqdm为循环添加进度条\n",
    "    for cif_file in tqdm(cif_files, desc=\"Converting files\"):\n",
    "        cif_path = os.path.join(input_cif_directory, cif_file)\n",
    "        pdb_file = cif_file.replace(\".cif\", \".pdb\")\n",
    "        pdb_path = os.path.join(output_pdb_directory, pdb_file)\n",
    "\n",
    "        obabel_path = path[\"obabel\"]\n",
    "        # 使用Open Babel将CIF转换为PDB\n",
    "        obabel_command = [obabel_path, cif_path, \"-O\", pdb_path]\n",
    "        with open(os.devnull, 'w') as devnull:\n",
    "            subprocess.run(obabel_command, stdout=devnull, stderr=devnull)\n",
    "\n",
    "        # 使用awk删除文件头\n",
    "        awk_command = [\"awk\", '/^ATOM |^TER/']\n",
    "        with open(pdb_path, \"rb\") as input_file:\n",
    "            awk_process = subprocess.run(awk_command, stdin=input_file, stdout=subprocess.PIPE, text=True)\n",
    "            tmp2_content = awk_process.stdout\n",
    "\n",
    "        with open(pdb_path, \"w\") as tmp2_file:\n",
    "            tmp2_file.write(tmp2_content)\n",
    "\n",
    "    print(\"转换完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chai-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
